<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LifeLearner: Hardware-Aware Meta Continual Learning System for Embedded Computing Platforms</title>
				<funder>
					<orgName type="full">Nokia Bell Labs</orgName>
				</funder>
				<funder ref="#_5dXjpqY">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder>
					<orgName type="full">Google Faculty Award</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,104.27,131.56,75.81,10.59"><forename type="first">Young</forename><forename type="middle">D</forename><surname>Kwon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.17,131.56,96.47,10.59"><forename type="first">Jagmohan</forename><surname>Chauhan</surname></persName>
							<email>j.chauhan@soton.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Southampton</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,449.27,131.56,42.75,10.59"><forename type="first">Hong</forename><surname>Jia</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,175.67,189.89,96.51,10.59"><forename type="first">Stylianos</forename><forename type="middle">I</forename><surname>Venieris</surname></persName>
							<email>s.venieris@samsung.com</email>
							<affiliation key="aff3">
								<orgName type="department">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.66,189.89,76.84,10.59"><forename type="first">Cecilia</forename><surname>Mascolo</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LifeLearner: Hardware-Aware Meta Continual Learning System for Embedded Computing Platforms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E6DA8A583F0FF6C25D13E47A0282EA3C</idno>
					<idno type="DOI">10.1145/3625687.3625804</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-02-04T20:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Continual Learning</term>
					<term>Meta Learning</term>
					<term>On-device Training</term>
					<term>Latent Replay</term>
					<term>Product Quantization</term>
					<term>Edge Computing</term>
					<term>Microcontrollers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Continual Learning (CL) allows applications such as user personalization and household robots to learn on the fly and adapt to context. This is an important feature when context, actions, and users change. However, enabling CL on resource-constrained embedded systems is challenging due to the limited labeled data, memory, and computing capacity.</p><p>In this paper, we propose LifeLearner, a hardware-aware meta continual learning system that drastically optimizes system resources (lower memory, latency, energy consumption) while ensuring high accuracy. Specifically, we (1) exploit meta-learning and rehearsal strategies to explicitly cope with data scarcity issues and ensure high accuracy, (2) effectively combine lossless and lossy compression to significantly reduce the resource requirements of CL and rehearsal samples, and (3) developed hardware-aware system on embedded and IoT platforms considering the hardware characteristics.</p><p>As a result, LifeLearner achieves near-optimal CL performance, falling short by only 2.8% on accuracy compared to an Oracle baseline. With respect to the state-of-the-art (SOTA) Meta CL method, LifeLearner drastically reduces the memory footprint (by 178.7√ó), end-to-end latency by 80.8-94.2%, and energy consumption by 80.9-94.2%. In addition, we successfully deployed LifeLearner on two edge devices and a microcontroller unit, thereby enabling efficient CL on resource-constrained platforms where it would be impractical to run SOTA methods and the far-reaching deployment of adaptable CL in a ubiquitous manner. Code is available at https://github.com/theyoungkwon/LifeLearner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>‚Ä¢ Computer systems organization ‚Üí Embedded and cyber-physical systems; ‚Ä¢ Human-centered computing ‚Üí Ubiquitous and mobile computing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rise of embedded and Internet of Things (IoT) devices, the adoption of deep neural networks (DNN) has revolutionized various applications ranging from computer vision <ref type="bibr" coords="1,502.63,405.60,13.23,7.94" target="#b26">[27]</ref>, audio <ref type="bibr" coords="1,543.60,405.60,14.60,7.94" target="#b76">[77]</ref> and sensing applications <ref type="bibr" coords="1,407.05,416.56,13.22,7.94" target="#b53">[54]</ref>. However, in real-world setups, where a deployed model may need to dynamically learn new tasks (i.e., new classes or inputs) from users <ref type="bibr" coords="1,441.99,438.48,10.55,7.94" target="#b7">[8]</ref> and adapt to changing input distributions <ref type="bibr" coords="1,367.63,449.44,13.49,7.94" target="#b68">[69]</ref>, existing learning approaches often fail, due to the constrained nature of available resources on edge devices and catastrophic forgetting (CF) <ref type="bibr" coords="1,416.82,471.35,13.30,7.94" target="#b65">[66]</ref>. CF describes the situation when a deployed model is able to perform new tasks but forgets previously learned knowledge. Efficient Continual Learning (CL) systems that can learn new tasks from growing data streams <ref type="bibr" coords="1,491.49,504.23,9.27,7.94" target="#b7">[8,</ref><ref type="bibr" coords="1,503.00,504.23,10.29,7.94" target="#b70">71,</ref><ref type="bibr" coords="1,515.53,504.23,11.49,7.94" target="#b75">76]</ref> are now being recognized as an important step forward as they also enable many practical applications. For example, household robotic devices need to continually learn to recognize new objects, while smart appliances need to learn different voice commands.</p><p>Many CL approaches have been proposed in the literature, including regularization-based <ref type="bibr" coords="1,408.39,569.99,13.54,7.94" target="#b43">[44,</ref><ref type="bibr" coords="1,424.17,569.99,14.17,7.94" target="#b102">103]</ref>, dynamic architecture-based <ref type="bibr" coords="1,545.65,569.99,13.54,7.94" target="#b33">[34,</ref><ref type="bibr" coords="1,317.96,580.94,10.35,7.94" target="#b81">82,</ref><ref type="bibr" coords="1,331.63,580.94,14.22,7.94" target="#b100">101]</ref>, and rehearsal-based methods <ref type="bibr" coords="1,463.30,580.94,9.44,7.94" target="#b7">[8,</ref><ref type="bibr" coords="1,476.06,580.94,10.35,7.94" target="#b73">74,</ref><ref type="bibr" coords="1,489.73,580.94,10.20,7.94" target="#b80">81]</ref>. Among these, rehearsal-based methods largely alleviate the forgetting issue of a learned model. Nonetheless, they are excessively data-hungry as they require a large number of labeled samples to learn new information and to be stored as rehearsal samples <ref type="bibr" coords="1,504.34,624.78,13.45,7.94" target="#b70">[71]</ref>, incurring high computational and memory overheads.</p><p>Another stream of work has recently attempted to utilize metalearning <ref type="bibr" coords="1,349.55,657.66,14.60,7.94" target="#b28">[29]</ref> in CL to address the problem of the scarce labeled data. A number of Meta CL methods <ref type="bibr" coords="1,431.28,668.62,9.23,7.94" target="#b3">[4,</ref><ref type="bibr" coords="1,442.73,668.62,10.27,7.94" target="#b36">37,</ref><ref type="bibr" coords="1,455.21,668.62,11.47,7.94" target="#b54">55]</ref> relying on a few samples of new classes to adapt and learn have been proposed. However, Meta CL's performance degrades when many classes are added during deployment, leading to low scalability (refer to Figure <ref type="figure" coords="1,546.48,701.49,6.55,7.94" target="#fig_0">1a</ref>).</p><p>Additionally, state-of-the-art (SOTA) Meta CL methods, OML+AIM and ANML+AIM <ref type="bibr" coords="2,119.53,98.75,13.49,7.94" target="#b54">[55]</ref>, exhibit large memory footprint, easily exceeding the RAM size on many embedded devices (e.g., 1 GB) (refer to Figure <ref type="figure" coords="2,89.09,120.67,6.65,7.94" target="#fig_0">1b</ref>). Further, we observed that the end-to-end latency of SOTA Meta CL methods to continually learn multiple classes is computationally expensive. These aspects render prior Meta CL methods not deployable on resource-constrained devices. As such, there is an emerging need for novel system design approaches that facilitate the broader deployment of CL systems on various IoT devices by bringing down resource requirements of CL methods without jeopardizing their accuracy.</p><p>To address the aforementioned limitations, we develop Life-Learner, the first hardware-aware system that fully enables dataand memory-efficient CL on the constrained edge and IoT devices. First, contrary to the existing Meta CL methods that primarily rely on regularization and suffer from accuracy loss, we introduce rehearsal-based Meta CL; we co-design meta-learning with an efficient rehearsal strategy, enabling LifeLearner to rapidly learn new classes using only a few samples while alleviating catastrophic forgetting of the already learned classes upon deployment (Section 3.1). Second, we propose a CL-tailored algorithm/software co-design approach that minimizes the on-device resource overheads of CL. At the algorithmic level, we design a latent replay scheme, where rehearsal samples are extracted from an intermediate layer of the target DNN instead of holding copies of raw inputs. By strategically selecting the rehearsal layer for high compressibility, we facilitate the subsequent compression of rehearsal samples, enabling their efficient storage on-device. Besides, based on an observation that latent replays are sparse, we further design a novel Compression Module via an intelligent combination of lossless compression to utilize sparsity and lossy compression to yield a high compression rate, fast encoding and decoding, and minimal resource usage (Section 3.2). Finally, we develop our hardware-aware system by employing hardware-friendly optimization techniques and considering the unique characteristics of hardware (e.g., write operation on Flash of IoT devices is costly during runtime) to optimize the runtime efficiency of CL operations on-device (Section 4).</p><p>We make the following key contributions:</p><p>(1) A novel Meta CL method comprises a rehearsal strategy that alleviates catastrophic forgetting and a deployment-time inner-and outer-loop training structure that achieves both fast adaptation to new classes and refreshing of already learned classes. Life-Learner achieves previously unattainable levels of on-device accuracy, outperforming all existing Meta CL methods by 4.1-16.1% on image and audio datasets, while being within 2.8% of an oracle. (2) A new algorithm/software co-design method that co-optimizes the rehearsal strategy and the compression pipeline to significantly reduce the resource requirements of CL and rehearsal samples. As a result, LifeLearner requires only 3.40-15.45 MB of memory and obtains a compression rate of 11.4-178.7√ó compared to the SOTA Meta CL method, ANML+AIM. This allows LifeLearner to run on edge devices, something impossible for current SOTA methods due to their large memory requirements (&gt;1.05 GB). (3) Our hardware-aware system implementation successfully deployed LifeLearner on two embedded devices (Jetson Nano and Raspberry Pi 3B+) and a microcontroller (STM32H747   <ref type="bibr" coords="2,443.78,526.15,13.40,7.94" target="#b38">[39,</ref><ref type="bibr" coords="2,458.90,526.15,10.27,7.94" target="#b47">48,</ref><ref type="bibr" coords="2,470.89,526.15,10.05,7.94" target="#b70">71]</ref>. In the literature, various approaches attempt to solve the forgetting problem <ref type="bibr" coords="2,518.77,537.11,13.40,7.94" target="#b12">[13,</ref><ref type="bibr" coords="2,534.04,537.11,10.27,7.94" target="#b64">65,</ref><ref type="bibr" coords="2,546.18,537.11,10.05,7.94" target="#b65">66]</ref>.</p><p>The first group of approaches includes regularization-based methods <ref type="bibr" coords="2,333.00,559.03,9.41,7.94" target="#b1">[2,</ref><ref type="bibr" coords="2,344.66,559.03,10.34,7.94" target="#b43">44,</ref><ref type="bibr" coords="2,357.24,559.03,10.34,7.94" target="#b83">84,</ref><ref type="bibr" coords="2,369.82,559.03,10.34,7.94" target="#b84">85,</ref><ref type="bibr" coords="2,382.41,559.03,14.32,7.94" target="#b102">103]</ref>: these add a regularization term to the loss function to minimize changes to important weights of a model for previously learned classes to prevent forgetting. This approach can be very efficient regarding computation and memory costs. However, it is shown to be less effective than other methods that utilize additional resources such as expanding architectures and storing additional samples <ref type="bibr" coords="2,386.84,624.78,13.22,7.94" target="#b12">[13]</ref>, as introduced in the following. The second group of approaches includes the dynamic architecture-based methods <ref type="bibr" coords="2,332.43,646.70,13.40,7.94" target="#b33">[34,</ref><ref type="bibr" coords="2,347.94,646.70,10.27,7.94" target="#b81">82,</ref><ref type="bibr" coords="2,360.32,646.70,15.64,7.94" target="#b100">101]</ref> that dynamically expand and freeze DNN architectures to incorporate new classes and prevent forgetting. Despite the promising performance, dynamic architectures pose the costly requirement of modifying the model architecture. This leads to higher computational costs as the model expands and prohibits the utilization of compile-time optimizations on a fixed computation graph of the model. The last group of approaches among conventional CL includes rehearsal-based methods <ref type="bibr" coords="3,188.72,98.75,9.23,7.94" target="#b6">[7,</ref><ref type="bibr" coords="3,200.00,98.75,6.10,7.94" target="#b7">8,</ref><ref type="bibr" coords="3,208.14,98.75,10.27,7.94" target="#b25">26,</ref><ref type="bibr" coords="3,220.45,98.75,10.27,7.94" target="#b48">49,</ref><ref type="bibr" coords="3,232.77,98.75,10.27,7.94" target="#b62">63,</ref><ref type="bibr" coords="3,245.08,98.75,10.27,7.94" target="#b66">67,</ref><ref type="bibr" coords="3,257.40,98.75,10.27,7.94" target="#b73">74,</ref><ref type="bibr" coords="3,269.71,98.75,10.27,7.94" target="#b80">81,</ref><ref type="bibr" coords="3,282.03,98.75,10.05,7.94" target="#b97">98]</ref>. These prevent forgetting by replaying the saved rehearsal samples from earlier classes, typically leading to superior CL performance over the other methods at the cost of increased memory footprint.</p><p>In this work, we opt to use a rehearsal-based method due to its primarily superior performance in CL settings and the avoidance of dynamic expansion of the model architecture during deployment, allowing us to apply system optimizations on the static computation graph of the model (see last paragraph of Section 4 for details).</p><p>Given a single trajectory of samples from a stream of classes T , minimizing the CL loss of a DNN that is trained end-to-end is more challenging than conventional DNN training <ref type="bibr" coords="3,250.61,219.30,13.41,7.94" target="#b36">[37]</ref>. This is because various complex challenges need to be solved together:</p><p>(1) the forgetting problem incurred when learning a stream of different classes, (2) the issue with the lack of labeled samples, and (3) training DNNs is extremely sample-inefficient: the minimization problem requires multiple training epochs to converge to a reasonable solution. Specifically, many CL methods <ref type="bibr" coords="3,251.42,285.05,13.61,7.94" target="#b47">[48,</ref><ref type="bibr" coords="3,267.94,285.05,11.59,7.94" target="#b70">71]</ref> are proposed to alleviate the forgetting problem. However, they require a large amount of labeled data (a few thousand) and many training epochs. Another learning approach, called meta-learning, is proposed to make DNN more sample-efficient <ref type="bibr" coords="3,216.96,328.89,13.40,7.94" target="#b14">[15,</ref><ref type="bibr" coords="3,231.95,328.89,10.27,7.94" target="#b28">29,</ref><ref type="bibr" coords="3,243.81,328.89,10.27,7.94" target="#b52">53,</ref><ref type="bibr" coords="3,255.68,328.89,10.05,7.94" target="#b98">99]</ref>, requiring only a few samples to adapt/learn new data distributions from a correlated data stream <ref type="bibr" coords="3,145.27,350.81,9.41,7.94" target="#b0">[1,</ref><ref type="bibr" coords="3,156.91,350.81,10.19,7.94" target="#b67">68]</ref>. However, existing meta-learning methods often neglect the forgetting problem of the already learned classes as it primarily aims at fast adaptation towards new tasks only <ref type="bibr" coords="3,72.40,383.68,9.33,7.94" target="#b8">[9,</ref><ref type="bibr" coords="3,83.98,383.68,10.31,7.94" target="#b18">19,</ref><ref type="bibr" coords="3,96.53,383.68,10.31,7.94" target="#b21">22,</ref><ref type="bibr" coords="3,109.08,383.68,10.31,7.94" target="#b23">24,</ref><ref type="bibr" coords="3,121.64,383.68,10.31,7.94" target="#b29">30,</ref><ref type="bibr" coords="3,134.19,383.68,10.31,7.94" target="#b78">79,</ref><ref type="bibr" coords="3,146.74,383.68,10.31,7.94" target="#b85">86,</ref><ref type="bibr" coords="3,159.30,383.68,10.13,7.94" target="#b92">93]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Meta Continual Learning</head><p>To overcome the challenges mentioned thus far, researchers proposed a novel approach, Meta CL, that utilizes meta-learning in CL to enable data-efficient and fast adaptation to new classes and also attempts to alleviate forgetting of already learned classes through novel ways of regularization and/or modification of the model architecture <ref type="bibr" coords="3,93.75,482.31,9.44,7.94" target="#b3">[4,</ref><ref type="bibr" coords="3,105.70,482.31,10.35,7.94" target="#b36">37,</ref><ref type="bibr" coords="3,118.57,482.31,10.21,7.94" target="#b54">55]</ref>. First, to enable fast adaptation with only a few samples, Meta CL methods are based on the training procedure of meta-learning. The meta-learning uses an outer loop and an inner loop where the outer loop takes steps to improve the learning ability of the inner loop that optimizes the DNN model with a few samples. This phase is called meta-training, which is typically performed on an offline server. The meta-training phase aims to find a better weight initialization of DNNs for fast adaptation with a few samples. After the meta-training is finished, the learned DNNs are tested given a few examples of new classes, referred to as the meta-testing phase, that could run on embedded systems. Secondly, to prevent the forgetting problem, Meta CL methods separate the network architecture into the feature extractor and the classifier. During the meta-training phase, Meta CL adopts the concept of fast and slow learning on an architecture level. The feature extractor is updated in the outer loop (slow weights) using random samples from learned classes to prevent forgetting. The classifier is updated in the inner loop (fast weights) to learn new classes swiftly. This approach has proven useful in alleviating CF <ref type="bibr" coords="3,219.45,679.57,9.33,7.94" target="#b3">[4,</ref><ref type="bibr" coords="3,231.03,679.57,10.31,7.94" target="#b36">37,</ref><ref type="bibr" coords="3,243.58,679.57,10.13,7.94" target="#b54">55]</ref>.</p><p>Although prior works in Meta CL enable CL with limited data samples, they have certain limitations. For example, Online-aware Meta-Learning (OML) <ref type="bibr" coords="3,399.93,87.79,14.67,7.94" target="#b36">[37]</ref> and A Neuromodulated Meta-Learning (ANML) <ref type="bibr" coords="3,351.75,98.75,10.68,7.94" target="#b3">[4]</ref> can retain high CL performance on the Omniglot dataset <ref type="bibr" coords="3,345.81,109.71,14.64,7.94" target="#b51">[52]</ref> over many classes. Also, Attentive Independent Mechanisms (AIM) module <ref type="bibr" coords="3,398.53,120.67,14.68,7.94" target="#b54">[55]</ref> captures independent concepts to learn new knowledge. In fact, AIM and its combinations, ANML+AIM and OML+AIM, have achieved SOTA results. However, as prior Meta CL only relies on inner-loop optimization in the meta-testing phase, it does not utilize the concept of learning fast and slow weights during deployment. Further, these methods fail to generalize (see Figure <ref type="figure" coords="3,548.66,175.46,6.78,7.94" target="#fig_0">1a</ref>; low accuracy on CIFAR-100 <ref type="bibr" coords="3,418.75,186.42,13.77,7.94" target="#b46">[47]</ref>) and have extremely high memory requirements (see Figure <ref type="figure" coords="3,414.50,197.38,6.75,7.94" target="#fig_0">1b</ref>), which limits their applicability to low-end devices. Hence, we aim to design an efficient Meta CL system that obtains high accuracy and less forgetting while making the practical deployment on embedded devices a reality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Efficient Deep Learning Systems</head><p>Scarce memory and compute resources are major bottlenecks in deploying DNNs on constrained embedded and IoT devices. In this context, researchers have largely focused on optimizing the inference stage (i.e., forward pass) by proposing lightweight DNN architectures <ref type="bibr" coords="3,338.32,317.93,13.44,7.94" target="#b19">[20,</ref><ref type="bibr" coords="3,354.00,317.93,10.29,7.94" target="#b55">56,</ref><ref type="bibr" coords="3,366.53,317.93,10.29,7.94" target="#b56">57,</ref><ref type="bibr" coords="3,379.06,317.93,10.29,7.94" target="#b63">64,</ref><ref type="bibr" coords="3,391.59,317.93,10.08,7.94" target="#b82">83]</ref>, pruning <ref type="bibr" coords="3,438.51,317.93,13.43,7.94" target="#b24">[25,</ref><ref type="bibr" coords="3,454.19,317.93,10.08,7.94" target="#b60">61]</ref>, quantization <ref type="bibr" coords="3,517.55,317.93,13.44,7.94" target="#b34">[35,</ref><ref type="bibr" coords="3,533.22,317.93,10.29,7.94" target="#b45">46,</ref><ref type="bibr" coords="3,545.75,317.93,10.08,7.94" target="#b79">80]</ref>, leveraging heterogeneous processors <ref type="bibr" coords="3,459.06,328.89,13.61,7.94" target="#b37">[38,</ref><ref type="bibr" coords="3,475.25,328.89,10.35,7.94" target="#b58">59,</ref><ref type="bibr" coords="3,488.18,328.89,10.21,7.94" target="#b59">60]</ref>, and offloading computation <ref type="bibr" coords="3,366.62,339.85,17.39,7.94" target="#b99">[100]</ref>.</p><p>In addition, many works focus on reducing the overall system resources required for DNN training <ref type="bibr" coords="3,457.62,361.77,101.57,7.94;3,317.96,372.72,101.96,7.94">[6, 11, 18, 21, 31-33, 36, 43, 51, 70, 73, 78, 87, 92, 96, 102]</ref>. For example, researchers control the layerwise growth of the model structure to enable efficient DNN training on mobile phones <ref type="bibr" coords="3,418.59,394.64,17.52,7.94" target="#b103">[104]</ref>. Other methods optimize sparse activations and redundant weights to avoid unnecessary storage of activations and weight updates during DNN training <ref type="bibr" coords="3,511.06,416.56,9.27,7.94" target="#b4">[5,</ref><ref type="bibr" coords="3,522.57,416.56,10.29,7.94" target="#b27">28,</ref><ref type="bibr" coords="3,535.09,416.56,10.08,7.94" target="#b57">58]</ref>. In particular, for memory-efficient training, researchers proposed efficient meta-learning approaches by tackling memory issues during meta-training <ref type="bibr" coords="3,371.91,449.44,14.85,7.94" target="#b91">[92]</ref> and meta-testing <ref type="bibr" coords="3,455.16,449.44,13.49,7.94" target="#b77">[78]</ref>. However, dynamically changing the updated parameters as in <ref type="bibr" coords="3,456.08,460.40,14.60,7.94" target="#b77">[78]</ref> is not suitable to be used for MCUs because Flash memory space where the model weights are stored is read-only during runtime, and SRAM is even more limited than Flash in terms of memory capacity. Thus, it is difficult to incorporate the dynamic parameter update on MCUs. Also, prior work <ref type="bibr" coords="3,339.00,515.19,14.75,7.94" target="#b44">[45]</ref> examines various lossless compression techniques (e.g., Huffman coding), which show at most a 3.3√ó compression ratio on activations. Lossy compression <ref type="bibr" coords="3,429.67,537.11,13.40,7.94" target="#b9">[10,</ref><ref type="bibr" coords="3,444.74,537.11,11.47,7.94" target="#b61">62]</ref> based on scalar quantization shows up to 12√ó memory savings without accuracy degradation. A promising method that can achieve even higher compression ratios (e.g., 128√ó) is Vector/Product Quantization (PQ) <ref type="bibr" coords="3,517.75,569.99,13.51,7.94" target="#b40">[41,</ref><ref type="bibr" coords="3,533.51,569.99,10.31,7.94" target="#b87">88,</ref><ref type="bibr" coords="3,546.07,569.99,10.13,7.94" target="#b88">89]</ref>. However, as it requires storing a separate codebook containing representative vectors, a brute-force utilization of PQ may not achieve actual memory savings. In this work, we demonstrate that PQ can be a key component towards efficient continuous learning and show how the on-device CL pipeline should be designed to accommodate it (see Section 3.2.2 and Figure <ref type="figure" coords="3,430.95,635.74,4.17,7.94" target="#fig_2">3</ref> for details).</p><p>In contrast to previous works, LifeLearner realizes efficient continual learning that was previously considered impractical for many embedded devices. By developing rehearsal-based Meta CL, effective algorithm/software co-design, and hardware-aware system implementation considering the unique characteristics of a wide range of embedded and IoT platforms (e.g., Jetson Nano, Pi 3B+, and STM32H747), LifeLearner yields both high accuracy and low resource overheads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LifeLearner</head><p>LifeLearner leverages the idea of Meta CL and rehearsal-based learning and minimizes the system overheads on embedded devices. Life-Learner consists of two phases. The first phase, i.e., meta-training, is performed on a server to obtain a good weight initialization by utilizing meta-learning in the CL setup with a few samples. The second phase is meta-testing: a meta-trained model is deployed on embedded devices and learns new classes continually without forgetting previously learned classes. Additionally, as shown in Figure <ref type="figure" coords="4,270.58,388.52,3.02,7.94" target="#fig_1">2</ref>, Life-Learner has two components to ensure superior performance and efficiency when it is deployed on resource-constrained devices:</p><p>(1) co-utilization of Meta CL and rehearsal strategy together with a deployment-time inner-and outer-loop optimization to resolve the accuracy degradation issue, (2) a design scheme that co-optimizes LifeLearner's rehearsal strategy and compression pipeline (Compression Module in Figure <ref type="figure" coords="4,152.64,465.24,3.49,7.94" target="#fig_1">2</ref>) to minimize the memory footprint, compute cost, and energy consumption when running CL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Co-utilization of Meta-Learning and Rehearsal Strategy</head><p>Current Meta CL methods rely on regularization in order to minimize radical changes to the already trained weights when learning new classes. As such, given a small set of training data from a stream of classes, all samples are discarded once they have been used. However, recent results from the CL literature <ref type="bibr" coords="4,247.73,569.99,14.78,7.94" target="#b12">[13]</ref> indicate that the alternative approach of rehearsal-based methods often outperforms regularization-based CL. Driven by this observation, we design our Meta CL method, called rehearsal-based Meta CL, which introduces a rehearsal strategy into the Meta CL to improve CL performance. Concretely, we introduce a Replay Buffer that stores informative samples from already learned classes; these serve as additional training samples when learning new classes, form a mechanism for refreshing the weights of the model, and avoid catastrophic forgetting.</p><p>In addition, existing Meta CL systems are limited by their sole use of inner-loop optimization during meta-testing. Instead, we construct a variant of the learning fast and slow weights approach: we utilize the samples of new classes during inner-loop updates to enable rapid adaptation to new classes, followed by outer-loop iterations with the rehearsal samples of the previously learned classes to alleviate catastrophic forgetting.</p><p>System Overhead. Despite the learning benefits of our rehearsalbased Meta CL method (see Section 5.2 for details), it comes at a system cost. With respect to memory, the Replay Buffer has to store a number of representative samples for each of the already encountered classes, so that they can be fetched during meta-testing. With respect to computation, the samples have to be processed by the DNN with both forward and backward passes to perform CL. Unless alleviated, these overheads can lead to a sharp increase in storage and computational requirements, hindering its deployment on mobile and embedded devices, where continual learning is most needed. In the next section, we present LifeLearner's co-design approach for alleviating these system costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CL-tailored Algorithm/Software Co-Design</head><p>To alleviate the system costs of rehearsal-based Meta CL and enable its deployment on resource-constrained devices, we present an algorithm-software co-design method, optimized for Continual Learning. At the algorithmic level, we design a rehearsal strategy that minimizes the computational overhead while maximizing the compressibility of the rehearsal samples. At the software level, we design a two-stage Compression Module that enables the efficient compression, storage and decompression of rehearsal samples, while inducing minimal on-device resource usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Rehearsal Strategy. Key design decision in rehearsal-based</head><p>methods constitutes the form of the rehearsal samples. A standard approach followed by many CL methods <ref type="bibr" coords="4,475.08,602.86,9.43,7.94" target="#b7">[8,</ref><ref type="bibr" coords="4,487.36,602.86,10.35,7.94" target="#b62">63,</ref><ref type="bibr" coords="4,500.57,602.86,11.59,7.94" target="#b80">81]</ref> is native rehearsal (i.e., raw data replay), which stores and replays the input data in their raw format, e.g., images are stored for computer vision tasks and MFCC features for audio tasks. Under this scheme, a random subset of the given classes is stored as rehearsal samples, which are later replayed to mitigate the forgetting issue. The drawbacks of this approach are the significant computational overhead, as the samples have to be processed from the full model, and the compression variability as compressibility varies substantially in a per-sample manner.  To counteract these drawbacks, we introduce latent replay into our rehearsal strategy. Under this scheme, instead of holding copies of raw inputs, we store their latent representations, i.e., intermediate activations at the output of a selected layer of the target DNN. In LifeLearner, we employ two techniques in order to enable the utilization of latent replay: i) select the last layer of the model's feature extractor as the rehearsal point; and ii) we freeze the feature extractor upon deployment and perform CL only on the classifier. With the feature extractor frozen, we render latent replay functionally equivalent to raw data replay. On the computational front, the forward pass of the feature extractor can be omitted when replaying latent representations and the backward propagation is performed until the last layer, inducing significant computational gains.</p><p>On the memory front, we make the following observation. In DNN training, the activations for each layer are saved during the forward propagation so that those activations are utilized for computing the gradients during the backward propagation. As in <ref type="bibr" coords="5,278.34,425.30,13.35,7.94" target="#b86">[87]</ref>, storing activations requires a large memory footprint depending on the batch size used for training. However, commonly used ReLU non-linearity in many DNN models results in sparse activations in the successive layers. Also, we observe that more than 90% of the activation values of the latent layer are zero due to the usage of ReLU from our analysis of the network architecture on all three datasets. By strategically selecting the rehearsal layer in the DNN and treating ReLU activations as the rehearsal samples, LifeLearner's rehearsal strategy facilitates their compression and subsequent efficient storage on-device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Compression Module for Latent Replays</head><p>We now introduce the Compression Module that is responsible for i) compressing rehearsal samples (i.e., latent activations in our work) when new classes are encountered and storing them in the Replay Buffer, and ii) fetching and decompressing them to perform CL at runtime. This component comprises two stages: sparse bitmap compression and product quantization (PQ).</p><p>Sparse Bitmap Compression. To leverage the sparsity of our latent replays for efficient storage, we employ sparse bitmap compression <ref type="bibr" coords="5,88.05,668.62,13.49,7.94" target="#b27">[28]</ref>. This scheme enables the Compression Module in LifeLearner to filter out the majority of zero values (typically 90% or more) in latent activations and save the remaining non-zero values to increase the compression rate for saving latent activations.</p><p>Figure <ref type="figure" coords="5,353.43,249.95,4.17,7.94" target="#fig_2">3</ref> depicts the compression and decompression processes. For compression, when latent activations are given to our system, a bitmap with the same dimensions as the latent activations sets a bit to 1 for non-zero values' indices and 0 for the remainders. Then, non-zero values and the sparse bitmap are stored in 32-bit floats and the bitmap format, respectively. For decompression, we traverse all elements of the bitmap and a vector containing the stored non-zero values, reconstructing in this process the latent activations by using either the saved non-zero value or zero if a bitmap element is 1 or 0, respectively. The compression and decompression processes are linear in runtime: ùëÇ (ùëõ), where ùëõ is the total number of elements of latent activations. With respect to memory, the footprint is reduced from (4ùëõ) when a dense format is used for storing latent activations to (4 √ó number of non-zero values + 1  8 ùëõ) with the bitmap. Product Quantization. To further minimize the resource overhead of rehearsal samples, we introduce a second stage to our compressor (Figure <ref type="figure" coords="5,391.93,425.46,3.49,7.94" target="#fig_2">3</ref>) utilizing PQ <ref type="bibr" coords="5,447.98,425.46,13.48,7.94" target="#b40">[41]</ref>. The output of the sparse bitmap compressor contains a vector of non-zero values. With PQ being a vector compression method that can compress a given vector v ‚àà R ùëë into ùë† number of PQ indices using a PQ codebook with ùë† columns, it is suitable to further reduce the size of the encoded rehearsal samples. Each column of the PQ codebook contains a set of representative vectors that well approximate ùë† sub-vectors of v when v is partitioned into ùë† sub-vectors.</p><p>For compression, the PQ encoder applies PQ to the non-zero activations v ‚àà R ùëë that are already filtered out by the first-stage sparse bitmap compression. We use 1 byte to store each PQ index and set ùëë/ùë† = {128, 32, 8} (length of each sub-vector). Then, each sub-vector of length ùëë/ùë† containing 32-bit floats is encoded to a 1-byte PQ index via our PQ encoder for more analysis regarding hyper-parameters). LifeLearner learns the PQ codebook offline using the latent activations during the meta-training phase, which is then stored on-device. For decompression, the PQ decoder reconstructs the non-zero activations v ‚Ä≤ using the stored PQ indices and the PQ codebook.</p><p>Finally, as in Algorithm 2 (see Lines 7, 9, and 10), our compression module is seamlessly incorporated in the inner-and outer-loop optimization of LifeLearner, enabling on-the-fly compression of the latent activations during deployment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Putting It All Together</head><p>Having described the main components of LifeLearner we now present the complete meta-training and meta-testing procedures that take place offline and online, respectively.</p><p>Meta-Training Procedure. Algorithm 1 shows the procedure of meta-training of Rehearsal-based Meta CL, LifeLearner. Firstly, the meta-training process of rehearsal-based Meta CL is the same as that of Meta CL <ref type="bibr" coords="6,111.80,679.39,9.27,7.94" target="#b3">[4]</ref>. In detail, it is comprised of an inner loop inside an outer loop of optimization. In the inner loop, the classifier part is updated (fast weights, e.g., ùúÉ ùëÉùêøùëÅ for OML and ùúÉ ùëÉ,ùê∂ùêøùêπ for ANML, ùúÉ ùëÉùêøùëÅ ,ùëä for OML+AIM, and ùúÉ ùëÉ,ùê∂ùêøùêπ,ùëä for ANML+AIM) (Lines 4-5). The number of weight update iterations is determined by the number of samples ùëò (e.g., 10-30) of a given sample set, ùëÜ ùë°ùëüùëé ùëó , of a new class, T ùë° . After the ùëò sequential updates, the meta-loss in the outer loop (Line 6) is computed using all the given samples on the new class (ùëÜ ùë°ùëüùëé ùëó ) and randomly sampled samples from all the meta-training classes (ùëÜ ùëüùëéùëõùëë ). All the weights of DNN are updated through outer-loop gradient updates using an Adam optimizer <ref type="bibr" coords="6,543.05,164.51,13.22,7.94" target="#b41">[42]</ref>. The learning rates, ùõº for the inner loop and ùõΩ for the outer loop, are used as hyper-parameters.</p><p>Meta-Testing Procedure. After executing the meta-training phase on a server, our system is deployed on resource-constrained devices and evaluated on its ability to learn unseen classes in the meta-testing phase. Algorithm 2 shows the meta-testing phase of the rehearsal-based Meta CL. In prior Meta CL, the meta-testing procedure contains only inner-loop optimization without outerloop optimization, i.e., only fast weights except for slow weights are fine-tuned. In contrast, LifeLearner leverages the full potential of meta-learning by using both inner-and outer-loop optimization in the meta-testing phase. Specifically, our proposed meta-testing procedure starts with the inner-loop weight updates to learn new classes swiftly using a few samples (Lines 5-6), followed by the outer-loop weight updates to retain the knowledge on the previously learned classes using the replayed samples plus the new samples (Line 8). Note that although the outer-loop iteration could run multiple epochs, the performance converges after one or two epochs (refer to Section 5.4 for more analysis). Also, LifeLearner integrates the compression module that compresses (Lines 9-10) and decompresses (Line 7) the latent activations during outer-loop optimization, as described in Section 3.2.</p><p>Our Contribution. Our method conceptually leverages existing concepts. We solve the challenge of incorporating these concepts in a coordinated, efficient end-to-end system (as discussed in Section 2.3). We achieve higher accuracy than baselines while reducing the memory footprint drastically. Our key contributions are (1) co-designing the algorithmic innovation (rehearsal strategy) with an intelligent combination of lossless (bitmap) and lossy (PQ) compression to significantly reduce the resource requirements of CL and latent replay samples (Section 3), (2) successfully deploying LifeLearner end-to-end on two embedded devices and MCU on which many prior works fail to run (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Hardware-Aware System Implementation</head><p>We develop the first phase, meta-training, of Meta CL methods on a Linux server to initialize the neural weights that can enable fast adaptation during deployment scenarios. After that, for the second phase, meta-testing, (i.e., actual deployment scenarios), we implemented our hardware-aware system by considering the hardware capacity and unique runtime characteristics of our target devices:</p><p>(1) embedded and mobile systems such as Jetson Nano and Raspberry Pi 3B+, and (2) a microcontroller unit such as STM32H747. To further optimize the system efficiency, we adopt hardware-friendly optimization techniques in our implementation <ref type="foot" coords="6,491.02,656.82,3.38,6.44" target="#foot_0">1</ref>Embedded Device. Jetson Nano has a quad-core ARM Cortex-A57 processor, and 4 GB of RAM, while Pi 3B+ contains a quadcore ARM Cortex-A53 processor with 1 GB of RAM. Note that the free memory space of Jetson Nano and Pi 3B+ during idle time is roughly 1.7 GB and 600 MB, respectively, due to the memory footprints pre-occupied by background, concurrent applications, and an operating system. As software platforms, we employ Faiss (PQ Framework) <ref type="bibr" coords="7,99.61,164.51,14.60,7.94" target="#b39">[40]</ref> and PyTorch 1.8 (Deep Learning Framework) <ref type="bibr" coords="7,279.45,164.51,14.59,7.94" target="#b71">[72]</ref> to develop and evaluate the meta-training and meta-testing phases on embedded systems.</p><p>Microcontroller Unit (MCU). To demonstrate the feasibility of the broader deployment of CL systems at the extreme edge, we further optimized and developed LifeLearner on MCUs. We implemented the online component of LifeLearner using C++ on an STM32H747 device equipped with ARM Cortex M4 and M7 cores with 1MB SRAM and 2 MB eFlash in total. However, we only utilize one core (ARM Cortex M7), as most MCUs have one CPU core. Also, we restrict the usage space of SRAM and eFlash to 512 KB and 1 MB, respectively, to enforce stricter resource constraints (an order of magnitude smaller memory space than other embedded devices with larger than 1 GB RAM).</p><p>To deploy LifeLearner on MCUs effectively and efficiently, we addressed many technical challenges and considered hardware characteristics. First of all, the memory requirements of the MetaCL methods developed on embedded devices, including LifeLearner, far exceed the hardware capacity of a "high-end" MCU such as STM32H747 (refer to Section 5.2). Hence, we first searched for a smaller yet accurate architecture for MCUs by experimenting with various width modifiers <ref type="bibr" coords="7,143.21,394.64,13.50,7.94" target="#b55">[56,</ref><ref type="bibr" coords="7,158.96,394.64,10.31,7.94" target="#b56">57,</ref><ref type="bibr" coords="7,171.51,394.64,11.53,7.94" target="#b82">83]</ref> (see Section 5.5 for details).</p><p>We then implemented our Compression Module (sparse bitmap compression and PQ) to reduce memory usage of latent replay samples on SRAM. In particular, we consider hardware characteristics and constraints: (1) the write operation on the storage (Flash) of MCUs is costly <ref type="bibr" coords="7,107.80,449.44,13.22,7.94" target="#b89">[90]</ref>, and (2) Flash is read-only during runtime <ref type="bibr" coords="7,271.24,449.44,9.23,7.94" target="#b2">[3,</ref><ref type="bibr" coords="7,282.03,449.44,10.05,7.94" target="#b49">50]</ref>. Hence, in our MCU implementation of LifeLearner, to minimize the memory footprint and energy consumption required for latent replay, we first compress latent replay samples using our Compression Module and then store them on SRAM, which has more limited memory but is faster and cheaper to perform read/write operations on than Flash. Note that our learned PQ codebook, used to encode and decode the latent replay samples after sparse bitmap compression, is stored on Flash to leave more space for scarce resources of SRAM. Also, PQ codebooks are static once deployed; they can be stored on the read-only memory of Flash.</p><p>In addition, we rely on the TFLM framework <ref type="bibr" coords="7,227.45,569.99,14.63,7.94" target="#b11">[12]</ref> to perform inference of the feature extractor on MCUs. However, TFLM does not support training (i.e., backpropagation). We developed our Backpropagation Engine based on C/C++ using Eigen <ref type="bibr" coords="7,242.67,602.86,14.85,7.94" target="#b22">[23]</ref> as a data structure and matrix multiplication library. Based on our Backpropagation Engine, we construct the classifier part on the fly whose weights are allocated on SRAM and can be continually learned during deployment whenever more data for new classes become available. Our lightweight Backpropagation Engine enables the implementation of the first CL system on MCUs.</p><p>Lastly, the binary size of our Compression Module and Backpropagation Engine, excluding C++ Standard Library (STL) on an MCU, is only 80 KB, introducing minimal overhead on storage.</p><p>Hardware-friendly Optimization. We further optimize Life-Learner's CL operations on-device. By freezing the model's feature extractor during deployment, LifeLearner significantly reduces the computational cost for the already learned classes during replay by omitting the forward and backward passes. In addition, we utilize the hardware-friendly 8-bit integer arithmetic <ref type="bibr" coords="7,495.21,142.59,14.85,7.94" target="#b90">[91]</ref> by reducing the precision of weights/activations of the feature extractor from 32-bit floats to 8-bit integers, increasing the computation throughput and minimizing latency and energy. The scalar quantization scheme <ref type="bibr" coords="7,347.11,186.42,13.44,7.94" target="#b34">[35,</ref><ref type="bibr" coords="7,362.79,186.42,11.49,7.94" target="#b45">46]</ref> is used to minimize the information loss in quantization. Then, we utilize the QNNPACK <ref type="bibr" coords="7,466.21,197.38,14.85,7.94" target="#b16">[17]</ref> backend engine and TFLM to execute the quantized model on two embedded devices and MCUs, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation 5.1 Experimental Setup</head><p>We briefly describe our experimental setup in this subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Metrics</head><p>As in <ref type="bibr" coords="7,391.80,296.64,9.27,7.94" target="#b3">[4]</ref>, we use testing accuracy on unseen samples of all the new classes learned continually as a key performance metric, representing the generalization ability of CL systems. In addition, we measure the memory footprint (model parameters, optimizers, activations, and rehearsal samples), end-to-end training latency and energy consumption to continually learn all the given classes for a deployed DNN on embedded devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Datasets</head><p>We employ three datasets of two different data modalities in our evaluation.</p><p>CIFAR-100 <ref type="bibr" coords="7,374.14,406.14,14.89,7.70" target="#b46">[47]</ref>: Following <ref type="bibr" coords="7,433.68,405.91,13.34,7.94" target="#b54">[55]</ref>, we employ CIFAR-100 in our evaluation as it is widely used dataset. CIFAR-100 consists of 60,000 images of 100 classes. Each class has 500 train images and 100 test images. 70 classes are used for meta-training and the remaining 30 for meta-testing. During both meta-training and meta-testing, up to only 30 training images are sampled for training in each class, which holds for both MiniImageNet and GSCv2 datasets. Then, during meta-testing, a total of 900 samples are given to perform CL.</p><p>MiniImageNet <ref type="bibr" coords="7,387.56,493.81,14.76,7.70" target="#b94">[95]</ref>: Following <ref type="bibr" coords="7,446.28,493.58,13.22,7.94" target="#b54">[55]</ref>, we employ MiniImageNet containing 64 classes for meta-training and 20 classes for metatesting. Each class has 540 images for training and 60 images for testing. During meta-testing, a total of 600 samples are given.</p><p>GSCv2 <ref type="bibr" coords="7,356.50,537.64,14.78,7.70" target="#b96">[97]</ref>: To generalize our results to another data modality, we include Google Speech Command V2 (GSCv2) as it is a widely used audio dataset. GSCv2 consists of a total of 35 classes of different keywords. We use 25 classes for meta-training and 10 classes for meta-testing. Each class has 2,424 and 314 input data for training and testing, respectively. During meta-testing, 300 samples in total are given for CL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Baselines</head><p>We compare our system, LifeLearner, with five baseline systems as follows.</p><p>Oracle: The CL performance of Oracle represents the upper bound performance of the experiments. It is because Oracle has access to all the classes at once in an i.i.d. fashion and performs DNN training for many epochs until the performance converges.</p><p>Pretrained: This baseline initializes the model weights based on conventional DNN training without the meta-learning procedure. Then, it finetunes the weights using given samples in the meta-test phase, similar to prior Meta CL methods. OML+AIM <ref type="bibr" coords="8,107.74,323.37,14.76,7.70" target="#b54">[55]</ref>: This is a Meta CL method based on OML with an Attentive Independent Mechanisms (AIM) module, capturing independent concepts to learn new knowledge.</p><p>ANML <ref type="bibr" coords="8,93.66,356.24,10.66,7.70" target="#b3">[4]</ref>: It is the representative Meta CL method. As this method is often reported to outperform OML <ref type="bibr" coords="8,218.05,366.98,13.22,7.94" target="#b36">[37]</ref>, we only employ ANML in our evaluation. Also, note that the proposed components of LifeLearner build on top of ANML.</p><p>ANML+AIM <ref type="bibr" coords="8,115.11,400.08,14.87,7.70" target="#b54">[55]</ref>: ANML+AIM is a Meta CL method based on ANML with an AIM module. This baseline serves as the SOTA Meta CL method as it often outperforms other Meta CL methods including OML+AIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Model Architecture</head><p>LifeLearner employs the network architecture used in the prior CL works for a fair comparison <ref type="bibr" coords="8,269.72,463.46,9.44,7.94" target="#b3">[4,</ref><ref type="bibr" coords="8,281.82,463.46,10.21,7.94" target="#b54">55]</ref>. As in Figure <ref type="figure" coords="8,103.66,474.42,3.13,7.94" target="#fig_1">2</ref>, it consists of the feature extractor and the final classifier. For ANML-based model architectures, the feature extractor consists of a neuromodulatory network, ùëì ùúÉ ùëÅ ùëÄ , and a prediction network, ùëì ùúÉ ùëÉ , followed by the classifier part, ùëì ùúÉ ùê∂ùêøùêπ . The neuromodulatory and prediction networks are 3-layer convolutional networks with 112 and 256 channels, respectively. The classifier has a single fully-connected layer. In this case, LifeLearner utilizes the last layer of the feature extractor as the latent replay layer, following the natural structure of the ANML architecture. 2 The SOTA method, ANML+AIM, adds AIM layers ùëì ùúÉ ùëä between the feature extractor and the classifier, which alleviates forgetting and helps learn new classes. In addition, for OML and OML+AIM, the feature extractor has a 6-layer convolutional network with 112 channels, followed by the classifier of two fully-connected layers with an AIM module between the feature extractor and the classifier. Note that the model architectures deployed on embedded devices (i.e., Jetson Nano and Pi 3B+) and an MCU (i.e., STM32H747) are different due to the strict resource constraint on the MCU. Thus, a smaller version of 2 When targeting a different model architecture, the latent replay layer selection is a configurable design decision. We leave this investigation as future work.</p><p>the model architecture described above is adopted for the MCU deployment (see Section 5.5 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">Training Details</head><p>We followed the meta-training procedure used in prior Meta CL works <ref type="bibr" coords="8,429.21,349.68,9.44,7.94" target="#b3">[4,</ref><ref type="bibr" coords="8,441.22,349.68,10.35,7.94" target="#b36">37,</ref><ref type="bibr" coords="8,454.13,349.68,10.21,7.94" target="#b54">55]</ref>. For instance, we used a batch size of 1 and 64 for the inner-and outer-loop updates over 20,000 steps, respectively. We experimented with different learning rates for the inner loop and outer loop to obtain the meta-trained DNN that provides the best accuracy on a validation set. As a result, for CIFAR-100 and GSCv2 datasets, the inner-loop learning rate (ùõº) is set to 0.001, and the outer-loop learning rate (ùõΩ) is also set to 0.001. For the MiniImageNet dataset, the optimal settings are ùõº = 0.001 and ùõΩ = 0.0005. During the meta-testing phase, ten different learning rates are tried for all the methods, and the best-performing results are reported. Besides, to obtain the accuracy results of systems that perform replays, we experimented with batch sizes of 8 and 16 and observed little difference in CL performance. Thus, we employ a batch size of 8, as a smaller batch size reduces the memory footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results</head><p>Accuracy. We start by evaluating the CL performance (testing accuracy) of LifeLearner compared to the baselines on the employed datasets. Figure <ref type="figure" coords="8,376.45,559.03,4.12,7.94" target="#fig_3">4</ref> presents the accuracy results of the meta-testing phase. Pretrained serves as the lower bound. The low accuracy (24.4% on average for three datasets) of Pretrained demonstrates that the conventional transfer learning approach cannot address the challenging scenarios of learning new classes with only a few samples. ANML improves upon Pretrained, however, the improvement is marginal (i.e., average 9.9% accuracy gain compared to Pretrained but 18.9% accuracy drop on average compared to Oracle which shows the upper bound accuracy). Note that it is very challenging to achieve high testing accuracy even for Oracle as the number of available samples is very limited during meta-testing: all evaluated systems are given only 30 samples per class, accounting for only 2.57%, 1.74%, and 0.5% of all training samples during metatraining of CIFAR-100, MiniImageNet, and GSCv2, respectively.</p><p>Table <ref type="table" coords="9,76.89,85.73,3.38,7.70" target="#tab_3">1</ref>: The required memory footprint and the compression ratio for the baselines and our system to perform CL during the meta-testing phase on the three datasets. LifeLearner achieves near-optimal CL performance, falling short by only 2.8% accuracy compared to Oracle. Also, LifeLearner outperforms all the Meta CL methods with substantial accuracy gains of 4.1-16.1% on average for the three datasets. Specifically, LifeLearner shows almost no loss of accuracy, i.e., 0.2% for CIFAR-100 and 2.7% for MiniImageNet compared to Oracle. In contrast, ANML+AIM (i.e., the previous SOTA Meta CL method) shows notable accuracy drops (9.9% for CIFAR-100 and 10.7% for MiniImageNet). In the case of GSCv2, LifeLearner reveals a slight accuracy decline of 5.6% compared to Oracle, while ANML+AIM shows a minor 0.2% drop in accuracy relative to Oracle.</p><p>Although LifeLearner shows a slightly lower accuracy for GSCv2 than ANML+AIM, it still outperforms ANML+AIM by 4.1% on average over all datasets. In addition, LifeLearner is essentially designed for edge devices to require drastically lower system resources (memory, latency, and energy) than the previous SOTA. As explained in the following, the excessive resource overhead of ANML+AIM makes it unsuitable to operate on resource-constrained devices.</p><p>Peak Memory Footprint. We investigate the peak memory footprint required to perform CL. Precisely, we measure the memory space required to perform backpropagation and to store rehearsal samples. The memory requirement to perform backpropagation consists of three components: (1) model memory that stores model parameters, (2) optimizer memory that stores gradients and momentum vectors, and (3) activation memory that is comprised of the intermediate activations (stored for reuse during backpropagation). Then, the memory requirement for rehearsal samples is included.</p><p>Table <ref type="table" coords="9,85.77,515.19,4.16,7.94" target="#tab_3">1</ref> shows the peak memory footprint for various baselines and our system. First, the AIM variants (OML+AIM and ANML+AIM) require an enormous memory footprint of 135.2-1,051 MB and 608.2-1,562 MB, respectively, as their AIM module has many parameters. This required memory easily exceeds the RAM size of embedded devices such as Pi 3B+ (i.e., 1 GB) and barely fits on Jetson Nano. Conversely, baseline systems such as Pretrained, ANML, and Oracle show modest memory requirements, which are around 10.16-10.20 MB for GSCv2, 39.7-39.9 MB for CIFAR-100, and 474.5-475.0 MB for MiniImageNet. However, as shown earlier, Pretrained and ANML methods are not highly accurate, and Oracle does not support CL. In contrast, LifeLearner shows the impressive results that it only requires 15.45 MB for CIFAR-100, 136.7 MB for MiniImageNet, and 3.40 MB for GSCv2, demonstrating a very high compression rate of 70.8√ó, 11.4√ó, and 178.7√ó compared to ANML+AIM, respectively. Compared to Oracle, LifeLearner shows a tight range of the compression (2.5-3.5√ó), indicating that we can estimate the compression gain within this range agnostic to the dataset. End-to-end Latency &amp; Energy Consumption. We now examine the run-time system efficiency, i.e., end-to-end latency and energy consumption for the entire CL process, of our system and the baselines when deployed on the two embedded devices -Jetson Nano and Pi 3B+ as shown in Figure <ref type="figure" coords="9,456.49,526.15,3.13,7.94" target="#fig_4">5</ref>. To obtain the end-to-end latency, we include: (1) the time to load a pretrained model, (2) the time to train the model continually over all the given classes one by one, and (3) the time to compress and decompress the latent representations using our compression method (i.e., sparse bitmap compression and PQ).</p><p>We first measure the end-to-end latency of our system and the baselines on Jetson Nano CPU to perform CL over all the given classes with 30 samples per class. As shown in Figures <ref type="figure" coords="9,535.07,613.82,10.44,7.94" target="#fig_4">5a,</ref><ref type="figure" coords="9,549.01,613.82,6.79,7.94" target="#fig_4">5c</ref>, and 5e, LifeLearner enables a fast end-to-end latency (415 seconds for CIFAR-100, 1,373 seconds for MiniImageNet, and 84 seconds for GSCv2), which is 80.8-94.2% reduction of latency compared to ANML+AIM (e.g., 7,100 seconds for CIFAR-100 and 438 seconds for GSCv2). Note that ANML+AIM often crashes from running out of memory on Jetson Nano due to its excessive memory requirements (as shown in Figures <ref type="figure" coords="9,394.07,690.53,7.87,7.94" target="#fig_4">5c</ref> and<ref type="figure" coords="9,421.94,690.53,6.56,7.94" target="#fig_4">5d</ref>). Furthermore, compared to ANML which shares the same network architecture, LifeLearner introduces negligible overheads in terms of the overall latency (343s vs. 415s for CIFAR-100, 1,280s vs. 1,373s for MiniImageNet, and 79s vs. 84s for GSCv2). It is because although there exist some overheads on LifeLearner to perform the compression techniques like the sparse bitmap compression and PQ, the speed gains derived from using quantized neural weights and activations offset the overheads of compression techniques (refer to Section 5.3 for details). After having demonstrated the efficiency of LifeLearner on the Jetson Nano, we deployed our system on an even more resource-constrained device, Pi 3B+ (600-700 MB available memory). The end-to-end latency on Pi 3B+ largely stays similar to that on Jetson Nano as shown in Figure <ref type="figure" coords="10,115.44,208.34,3.07,7.94" target="#fig_4">5</ref>.</p><p>To measure the energy consumption, we first use Tegrastats on Jetson Nano to measure the power consumption. Then, we calculate the energy consumption by multiplying power consumption and the elapsed time for each end-to-end CL trial. Similar to the latency results, Figures <ref type="figure" coords="10,113.03,263.14,10.77,7.94" target="#fig_4">5b,</ref><ref type="figure" coords="10,126.38,263.14,7.26,7.94" target="#fig_4">5d</ref>, and 5f show that LifeLearner remarkably reduces the energy consumption by 80.9-94.2% (1.9kJ vs. 32.7kJ for CIFAR-100 and 0.4kJ vs. 2.0kJ for GSCv2) compared to ANML+AIM. Moreover, compared to ANML, LifeLearner shows small overheads of the additional energy consumption (1.6kJ vs. 1.9kJ for CIFAR-100, 5.9kJ vs. 6.3kJ for MiniImageNet, and 0.36kJ vs. 0.39kJ for GSCv2). In the case of Pi 3B+, it consistently consumes less energy than Jetson Nano. It is because while the end-to-end latency of the two embedded devices is similar, the power consumption profile on Pi 3B+ is lower than that on Jetson Nano, making Pi 3B+ a more energy-efficient option. A YOTINO USB power meter is used to obtain the power consumption on Pi 3B+.</p><p>Summary. Our result demonstrates that LifeLearner can effectively learn new classes in a continual manner based on only a few samples without experiencing catastrophic forgetting, i.e., it generalizes well to new samples of many classes unseen during the offline learning phase. Moreover, LifeLearner enables fast and energy-efficient CL on edge devices with significantly reduced memory footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>We perform an ablation study to investigate the role of each component of our system by incrementally adding our proposed components on top of the baseline system (ANML): (1) rehearsal strategy with inner-and outer-loop optimization (Latent), (2) sparse bitmap compression (Latent+Bit), ( <ref type="formula" coords="10,152.05,537.11,3.11,7.94">3</ref>) PQ (Latent+PQ), and ( <ref type="formula" coords="10,240.37,537.11,3.11,7.94">4</ref>) quantization (LifeLearner).</p><p>Effect of Rehearsal with Double-Loop Optimization. As shown in Table <ref type="table" coords="10,109.96,569.99,3.01,7.94" target="#tab_5">2</ref>, we find that our proposed rehearsal strategy with double-loop optimization drastically improves the accuracy (compare ANML vs Latent). For example, Latent increases the accuracy of ANML by 10.6-28.4% across all the datasets. Yet, Latent causes resource overheads on memory footprint, latency, and energy consumption compared to ANML, as Latent is a baseline CL system without our Compression Module.</p><p>Effect of Compression and Hardware-aware Implementation. The results of various CL systems such as Latent+Bit, La-tent+PQ, and Latent+Bit+PQ show that the proposed compression techniques for latent representations do not sacrifice the accuracy of the CL systems but reduce the overall memory footprint compared to Latent. Moreover, our Compression Module incurs small Overall, the ablation study reveals that the co-utilization of the rehearsal strategy with double-loop optimization, Compression Module, and hardware-friendly implementation effectively makes LifeLearner more accurate and efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Parameter Analysis</head><p>Next, we study the impact of the various hyper-parameters that could affect the performance of our system (see Figure <ref type="figure" coords="10,519.34,526.15,2.94,7.94">6</ref>).</p><p>The Number of Given Samples. We first examine the accuracy of LifeLearner according to the number of given samples per class (ranging from 10 to 30) as it would directly affect labeling effort of users (see Figure <ref type="figure" coords="10,382.16,569.99,6.57,7.94">6a</ref>). Apparently, the more samples are given for training, the higher the accuracy, which holds for both LifeLearner and Oracle. Even when only 10 samples per class are given to conduct training, the accuracy degradation of LifeLearner is relatively low (7-14%), indicating that LifeLearner can still perform reasonably well under extreme data scarcity. Also, the accuracy differences between LifeLearner and Oracle are small (e.g., 1-2% for CIFAR-100, 1-3% for MiniImageNet, and 5-9% for GSCv2), demonstrating that LifeLearner achieves the similar accuracy of Oracle. With 30 given samples, the accuracy difference is minimal: 2.8% on average (ranging from 1 to 5%).</p><p>The Number of Replay Epochs. We study to what extent the number of replay epochs affects the CL performance as more epochs Figure <ref type="figure" coords="11,128.17,263.09,3.45,7.70">6</ref>: The parameter analysis of LifeLearner for all the datasets according to the three parameters.</p><p>incur larger latency and energy consumption. Figure <ref type="figure" coords="11,244.55,291.06,8.42,7.94">6b</ref> shows that the accuracy of LifeLearner converges after the first or the second replay epoch. However, Oracle requires at least two to five epochs to reach the convergence accuracy, which consumes much more training time and energy than our system (see Figure <ref type="figure" coords="11,245.24,334.89,2.88,7.94" target="#fig_4">5</ref>). This result benefits us since replaying the rehearsal samples over one or two epochs is enough for LifeLearner to reach the converging accuracy, which helps decrease the system overheads. PQ Codebook's Sub-vector Length. We investigate the accuracy of LifeLearner according to the sub-vector length of the PQ codebook (the number of values per index) ranging from 8 to 128 as it affects the compression ratio of rehearsal samples. For CIFAR-100 and MiniImageNet, there is little difference according to the sub-vector length. In contrast, for GSCv2, we observe that the shorter the length of the sub-vector (i.e., lower compression rate), the higher the accuracy. These results inform us to select the largest sub-vector length that does not degrade accuracy.</p><p>These results show that with only 10-30 samples per class, Life-Learner achieve similar CL performance to Oracle, exhibit rapid convergence with small replay epochs (at most two), and accomplish a high compression rate for rehearsal samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">MCU Deployment</head><p>TinyANML Architecture. For the extremely resource constrained IoT devices like MCUs where on-chip memory of SRAM and Flash are typically a few hundred KB or 1 MB at most (an order of magnitude smaller than Jetson Nano and Pi 3B+ in terms of memory), the memory requirements of the MetaCL methods, including Life-Learner, are prohibitively large. Thus, we propose a small and accurate TinyANML architecture designed for MCUs with tiny memory by experimenting with various width modifiers <ref type="bibr" coords="11,235.99,624.78,13.61,7.94" target="#b55">[56,</ref><ref type="bibr" coords="11,252.41,624.78,10.35,7.94" target="#b56">57,</ref><ref type="bibr" coords="11,265.56,624.78,10.21,7.94" target="#b82">83]</ref>. We identified widths of 0.2, 0.05, and 0.4 for the ANML architecture of CIFAR-100, MiniImageNet, and GSCv2, respectively.</p><p>MCU Implementation and Results. Backbone represents an inference-only feature extractor based on TFLM. On top of that, our hardware-aware systems are added incrementally: (1) Backpropagation Engine (Tiny ANML) and (2) Compression Module (Tiny LifeLearner). Table <ref type="table" coords="11,124.23,701.49,4.13,7.94" target="#tab_6">3</ref> shows the MCU deployment results based on STM32H747 in terms of accuracy, SRAM, Flash, latency, and energy consumption to learn a class with ten samples when continually learning ten classes. Backpropagation Engine. As shown with Tiny ANML compared to inference-only Backbone, our Backpropagation Engine enables on-device CL with extremely small latency/energy overheads (e.g., 579ms vs. 561ms and 134mJ vs. 128mJ for CIFAR-100) while requiring only an additional 100KB SRAM and 260KB Flash.</p><p>Co-design of Our Algorithm and Hardware-aware System Implementation. Tiny LifeLearner not only largely prevents accuracy degradation compared to its original LifeLearner (see Table <ref type="table" coords="11,551.99,559.03,3.38,7.94" target="#tab_5">2</ref>) but also maintains higher accuracy than ANML despite Tiny Life-Learner's model size being 24.1-1839√ó smaller than ANML. Tiny LifeLearner achieves significantly higher accuracy than Tiny ANML while having minimal resource requirements (e.g., 181-281kB SRAM, 725-825kB Flash, 832-1,204ms latency, and 195-282mJ energy consumption), demonstrating the effectiveness of our proposed algorithm and hardware-aware system implementation on such an extremely resource-constrained device.</p><p>Note that it is infeasible to perform the ablation study to quantify the benefits of our design as in Section 5.3. This is because other baselines with rehearsal strategy and prior works exhibit out-of-memory problems and only tiny LifeLearner could run on MCUs with severely limited memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Impact on Continual Learning. We envision that LifeLearner could make CL a practical reality on embedded and IoT devices by leveraging meta-learning and rehearsal strategy with only a few samples. Such CL systems will allow DNNs to add new classes (e.g., adding new objects to an image recognition system, adding new keywords to a voice assistant) or new modalities (e.g., adding image recognition on top of a voice recognition authentication system) on the fly without relying on the cloud (i.e., no communication costs). As one future direction, further optimizing LifeLearner to use stricter quantization such as 1, 2, or 4 bits will be interesting.</p><p>Generalizability of LifeLearner. LifeLearner successfully works on three different datasets operating on two different modalities: image and audio, showing the generalizability of our framework. With the proliferation of smart spaces, such as smart homes and offices, LifeLearner can be used to learn the personal habits and preferences of users in order to control environmental conditions, such as temperature, humidity and lighting, with readings coming from thermometers, motion sensors and cameras on IoT devices. LifeLearner would enable this personalization and space adaptivity to happen in a data-efficient manner and to stay local to ensure privacy. Moreover, LifeLearner could be used on robot vacuum cleaners to enhance their adaptability, e.g., to continually learn to visually recognize new objects and thus avoid collisions.</p><p>The evaluation of other datasets and potentially other modalities, including various other sensor signals <ref type="bibr" coords="12,214.56,364.51,13.59,7.94" target="#b13">[14,</ref><ref type="bibr" coords="12,230.39,364.51,11.58,7.94" target="#b74">75]</ref> as mentioned above to further test the applicability of LifeLearner for learning continually for other real-world applications, is left as future work.</p><p>Scalibility over Many Classes. The sample-wise compression ratio of LifeLearner is about 30√ó, significantly reducing the memory overhead of adding many classes. It incurs only 1.68 MB, 6.16 MB, and 0.66 MB of memory when adding 100 classes with 30 samples per class for CIFAR-100, MiniImageNet, and GSCv2, respectively. Also, our scalar quantization and selective layer updates resolve scalability issues of latency as it incurs minimal latency overhead over ANML with fixed latency to learn new classes (see Tables <ref type="table" coords="12,278.69,474.09,6.02,7.94" target="#tab_5">2,</ref><ref type="table" coords="12,286.79,474.09,2.88,7.94" target="#tab_6">3</ref>).</p><p>Feasibility of Labeling Samples. One of the key challenges of enabling realistic applications for CL is annotation difficulty by users. As conventional CL typically demands a few thousand labeled samples, it becomes almost infeasible for users to perform labeling (as discussed in Section 2.1). Instead, LifeLearner ameliorates this labeling burden by enabling data-efficient CL with 10-30 samples per class which are not impractical to label.</p><p>Other Considerations. In this work, our evaluation demonstrated that LifeLearner achieves near-optimal CL performance, falling short by only 2.8% accuracy compared to the upper bound system (Oracle). However, a higher accuracy (over 80-90%) given fewer samples (less than 10-30 samples) would be desirable. Thus, it is worth investigating larger and more advanced model architectures specializing in the target problem and task, such as Transformers <ref type="bibr" coords="12,84.86,638.48,13.55,7.94" target="#b15">[16,</ref><ref type="bibr" coords="12,100.66,638.48,10.16,7.94" target="#b93">94]</ref>, to push the envelope of the upper bound testing accuracy of the challenging CL problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We proposed LifeLearner, a hardware-aware meta CL system with adaptive fast-slow weights and resource-optimized compression for embedded and IoT platforms. LifeLearner outperforms all existing Meta CL methods by a large margin (approximating the upper bound method that performs training in i.i.d. setting) and demonstrates its potential applicability in real-world deployments. Our efficient CL system opens the door to adaptive applications to run on embedded and IoT devices by allowing them to learn new tasks and adapt to the dynamics of the user and context.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,317.96,217.56,240.25,7.70;2,317.68,228.52,242.13,7.70;2,317.96,239.48,239.61,7.70;2,317.96,250.44,240.50,7.70;2,317.96,261.40,241.85,7.70;2,317.96,272.36,131.11,7.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Preliminary analysis of the prior Meta CL methods (i.e., ANML, OML+AIM, ANML+AIM). (a) shows the CL accuracy degradation of the Meta CL methods after learning ùëê number of classes on CIFAR-100<ref type="bibr" coords="2,449.39,250.44,14.68,7.70" target="#b46">[47]</ref>. (b) shows the memory footprint needed to run the Meta CL methods on MiniIma-geNet<ref type="bibr" coords="2,343.48,272.36,16.34,7.70" target="#b94">[95]</ref> with a batch size of 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,53.80,219.63,505.49,7.70;4,53.80,230.59,504.40,7.70;4,53.52,241.55,326.91,7.70"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The system overview. LifeLearner consists of the frozen/quantized feature extractor, the continually learned classifier, and the compression module based on sparse bitmap and PQ. The compression module takes the feature extractor's outputs (activations) as inputs and compresses them to be saved as latent replay samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,53.80,200.07,504.40,7.70;5,53.80,211.03,504.40,7.70;5,53.80,221.99,457.67,7.70"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The overview of our compression module. It consists of (1) a sparse bitmap to filter out zero from activations or to reconstruct decompressed activations from non-zero activations, (2) a PQ encoder that further compresses non-zero activations into PQ indices, and (3) a PQ decoder that decompresses PQ indices back into decompressed non-zero activations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,53.80,262.29,504.60,7.70;8,53.80,273.25,196.50,7.70"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The accuracy of the CL systems on the three datasets of two different modalities. Reported results are averaged over three trials, and standard-deviation intervals are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,317.96,410.79,240.25,7.70;9,317.96,421.75,240.25,7.70;9,317.96,432.71,240.25,7.70;9,317.96,443.67,81.80,7.70"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The end-to-end latency and energy consumption of the baselines and LifeLearner to perform CL over all the given classes. All results are averaged over three runs with standard deviations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="11,80.99,242.32,128.11,5.99;11,246.77,242.32,118.25,5.99;11,419.95,242.32,93.58,5.99"><head></head><label></label><figDesc>(a) The Number of Samples per Class (S) (b) The Number of Replay Epochs (E) (c) The Sub-Vector Length (L)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,109.59,104.05,392.70,78.52"><head>Compression Module (Sparse Bitmap &amp; PQ) Sparse Bitmap Non-Zero Activations PQ Encoder PQ Indices</head><label></label><figDesc></figDesc><table coords="5,109.59,122.32,392.70,60.24"><row><cell>From Feature Extractor Latent Activations (z)</cell><cell>Non-Zero Decompressed PQ Decoder</cell><cell>Bitmap Sparse</cell><cell>To Classifier Decompressed Latent Activations (z')</cell></row><row><cell></cell><cell>Activations</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,53.13,88.94,228.16,479.91"><head>Algorithm 1 :</head><label>1</label><figDesc>Meta-Training Procedure of LifeLearner Require: ùëÅ sequential classes T ; learning rates (LR) ùõº, ùõΩ; inner-loop iterations ùëò; modules ùëì ùúÉ , ùúô ùúÉ ; given samples ùëÜ Update slow weights using {ùëÜ ùë°ùëüùëé ùëó , ùëÜ ùëüùëéùëõùëë } ‚ñ∑ LR: ùõΩ ùëÜ ùëôùëéùë°ùëíùëõùë° = ùëì ùúÉ ùëÅ ùëÄ (ùëÜ ùë°ùëüùëé ùëó ) ‚äô ùëì ùúÉ ùëÉ (ùëÜ ùë°ùëüùëé ùëó ) // Store compressed activations for rehearsal 10 ùëÜ ùëüùëí‚Ñéùëíùëéùëüùë†ùëéùëô = {ùëÜ ùëüùëí‚Ñéùëíùëéùëüùë†ùëéùëô , ùêµùëñùë°ùëÉùëÑ ùëêùëúùëöùëùùëüùëíùë†ùë† (ùëÜ ùëôùëéùë°ùëíùëõùë° )} 11 ùëÜ ùë°ùëíùë†ùë° = T -ùëÜ ùë°ùëüùëéùëñùëõ // Held-out test set 12 Evaluate on ùëÜ ùë°ùëüùëéùëñùëõ , ùëÜ ùë°ùëíùë†ùë° // Eval on training/test set</figDesc><table coords="6,56.60,136.52,224.69,383.21"><row><cell></cell><cell cols="3">// Outer-loop starts here</cell></row><row><cell></cell><cell cols="4">// Inner-loop starts here</cell></row><row><cell>3</cell><cell cols="2">for ùëñ = 1, ..., ùëò do</cell><cell></cell></row><row><cell>4</cell><cell cols="4">Update fast weights using ùëÜ ùë°ùëüùëé ùëó</cell><cell>‚ñ∑ LR: ùõº</cell></row><row><cell></cell><cell cols="2">/* OML(+AIM):ùúô</cell><cell cols="2">ùúÉ ùëÉùêøùëÅ ( ùëì ùúÉ ùëä ),</cell></row><row><cell></cell><cell cols="3">ANML(+AIM):ùëì ùúÉ ùëÉ , ùúô</cell><cell>ùúÉ ùê∂ùêøùêπ (ùëì ùúÉ ùëä ), LifeLearner:ùúô</cell><cell>ùúÉ ùê∂ùêøùêπ</cell><cell>*/</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">/* OML(+AIM):ùëì ùúÉ ùëÖùêøùëÅ , ANML(+AIM):ùëì ùúÉ ùëÅ ùëÄ , ùëì ùúÉ ùëÉ , ùúô</cell><cell>ùúÉ ùê∂ùêøùêπ ,</cell></row><row><cell></cell><cell cols="4">LifeLearner:ùëì ùúÉ ùëÅ ùëÄ , ùëì ùúÉ ùëÉ , ùúô</cell><cell>ùúÉ ùê∂ùêøùêπ</cell></row><row><cell></cell><cell cols="2">/* OML(+AIM):ùúô</cell><cell cols="2">ùúÉ ùëÉùêøùëÅ ( ùëì ùúÉ ùëä ),</cell></row><row><cell></cell><cell cols="3">ANML(+AIM):ùëì ùúÉ ùëÉ , ùúô</cell><cell>ùúÉ ùê∂ùêøùêπ (ùëì ùúÉ ùëä ), LifeLearner:ùúô</cell><cell>ùúÉ ùê∂ùêøùêπ</cell><cell>*/</cell></row><row><cell></cell><cell cols="4">// Get latent activations from compressed rehearsal samples</cell></row><row><cell></cell><cell cols="4">/* OML(+AIM):ùëì ùúÉ ùëÖùêøùëÅ , ANML(+AIM):ùëì ùúÉ ùëÅ ùëÄ , ùëì ùúÉ ùëÉ , ùúô</cell><cell>ùúÉ ùê∂ùêøùêπ ,</cell></row><row><cell></cell><cell>LifeLearner:ùúô</cell><cell cols="2">ùúÉ ùê∂ùêøùêπ</cell><cell>*/</cell></row><row><cell></cell><cell cols="4">// Get latent activations</cell></row><row><cell>9</cell><cell></cell><cell></cell><cell></cell></row></table><note coords="6,56.21,145.82,72.70,8.24;6,56.60,159.18,3.07,5.13;6,77.34,159.01,17.42,5.34;6,98.46,156.59,14.45,7.70;6,110.73,159.01,28.44,5.96;6,142.75,156.59,14.45,7.70;6,274.32,249.19,6.97,5.61;6,58.45,286.27,195.63,7.94;6,63.76,298.86,203.85,8.43;6,100.43,310.31,175.77,7.94;6,277.49,310.31,1.97,7.94;6,100.61,323.20,92.65,5.96;6,194.29,321.26,39.60,7.94;6,56.21,335.53,28.36,5.34;6,87.82,333.11,56.27,8.38;6,147.77,333.11,15.84,7.93;6,63.41,346.01,87.17,5.61;6,56.21,355.32,72.70,8.24;6,56.60,368.67,3.07,5.13;6,77.34,368.51,17.42,5.34;6,98.46,366.09,14.45,7.70;6,110.73,370.61,2.55,3.24;6,56.60,380.63,3.07,5.13;6,77.34,380.46,21.08,5.34;6,101.66,378.04,15.73,7.93;6,117.50,380.46,38.09,5.34;6,157.04,378.04,3.69,7.70;6,77.26,390.95,87.17,5.61;6,56.60,402.64,3.07,5.13;6,77.61,400.25,61.98,8.24;6,57.00,413.60,3.07,5.13;6,91.46,411.50,114.17,7.94;6,233.75,411.50,26.76,8.10;6,56.60,458.33,3.07,5.13;6,77.34,458.16,23.78,5.96;6,104.85,455.93,68.15,8.19;6,174.48,455.74,7.15,7.70;6,181.70,460.88,30.81,3.24;6,213.70,455.74,2.99,7.70;6,56.60,470.66,3.07,5.13;6,77.61,468.07,108.77,8.43;6,186.48,470.49,41.26,5.96;6,229.21,468.07,32.77,8.58"><p>1 for ùë° = 1, ..., ùëÅ do 2 ùëÜ ùë°ùëüùëé ùëó ‚àº T ùë° , ùëÜ ùëüùëéùëõùëë ‚àº T */ Algorithm 2: Meta-Testing Procedure of LifeLearner Require: ùëÅ sequential unseen classes T ; learning rates (LR) ùõº, ùõΩ; inner-loop iterations ùëò; modules ùëì ùúÉ , ùúô ùúÉ , ùêµùëñùë°ùëÉùëÑ ùëêùëúùëöùëùùëüùëíùë†ùë†,ùëëùëíùëêùëúùëöùëùùëüùëíùë†ùë† ; samples ùëÜ 1 ùëÜ ùë°ùëüùëéùëñùëõ = {}, ùëÜ ùëüùëí‚Ñéùëíùëéùëüùë†ùëéùëô = {} // Outer-loop starts here 2 for ùë° = 1, ..., ùëÅ do 3 ùëÜ ùë°ùëüùëé ùëó ‚àº T ùë° 4 ùëÜ ùë°ùëüùëéùëñùëõ = {ùëÜ ùë°ùëüùëéùëñùëõ , ùëÜ ùë°ùëüùëé ùëó } // Inner-loop starts here 5 for ùëñ = 1, ..., ùëò do 6 Update fast weights using ùëÜ ùë°ùëüùëé ùëó ‚ñ∑ LR: ùõº 7 ùëÜ ùëôùëéùë°ùëíùëõùë° = ùêµùëñùë°ùëÉùëÑ ùëëùëíùëêùëúùëöùëùùëüùëíùë†ùë† (ùëÜ ùëüùëí‚Ñéùëíùëéùëüùë†ùëéùëô ) 8 Update slow weights using {ùëÜ ùë°ùëüùëé ùëó , ùëÜ ùëôùëéùë°ùëíùëõùë° } ‚ñ∑ LR: ùõΩ</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,317.66,85.73,242.06,358.41"><head>Table 2 :</head><label>2</label><figDesc>The comparison of LifeLearner and variants of rehearsal-based Meta CL methods for ablation study.</figDesc><table coords="10,317.96,121.96,241.76,322.18"><row><cell>Dataset</cell><cell>System</cell><cell cols="4">Accuracy Memory Latency Energy</cell></row><row><cell></cell><cell>ANML</cell><cell>0.272</cell><cell>39.7 MB</cell><cell>343.2s</cell><cell>1.58kJ</cell></row><row><cell></cell><cell>Latent</cell><cell>0.452</cell><cell>53.9 MB</cell><cell>432.5s</cell><cell>1.99kJ</cell></row><row><cell>CIFAR-100</cell><cell>Latent+Bit Latent+PQ</cell><cell>0.452 0.448</cell><cell>41.2 MB 41.8 MB</cell><cell>466.9s 437.1s</cell><cell>2.15kJ 2.01kJ</cell></row><row><cell></cell><cell>Latent+Bit+PQ</cell><cell>0.446</cell><cell>40.4 MB</cell><cell>471.4s</cell><cell>2.17kJ</cell></row><row><cell></cell><cell>LifeLearner</cell><cell>0.443</cell><cell>15.5 MB</cell><cell>414.7s</cell><cell>1.91kJ</cell></row><row><cell></cell><cell>ANML</cell><cell>0.327</cell><cell>474.5 MB</cell><cell>1,280s</cell><cell>5.89kJ</cell></row><row><cell></cell><cell>Latent</cell><cell>0.433</cell><cell>512.5 MB</cell><cell>1,492s</cell><cell>6.86kJ</cell></row><row><cell>Mini-</cell><cell>Latent+Bit</cell><cell>0.433</cell><cell>477.7 MB</cell><cell>1,551s</cell><cell>7.14kJ</cell></row><row><cell>ImageNet</cell><cell>Latent+PQ</cell><cell>0.430</cell><cell>483.0 MB</cell><cell>1,501s</cell><cell>6.90kJ</cell></row><row><cell></cell><cell>Latent+Bit+PQ</cell><cell>0.423</cell><cell>476.4 MB</cell><cell>1,560s</cell><cell>7.18kJ</cell></row><row><cell></cell><cell>LifeLearner</cell><cell>0.411</cell><cell>136.7 MB</cell><cell>1,373s</cell><cell>6.32kJ</cell></row><row><cell></cell><cell>ANML</cell><cell>0.429</cell><cell>10.2 MB</cell><cell>78.6s</cell><cell>0.36kJ</cell></row><row><cell></cell><cell>Latent</cell><cell>0.713</cell><cell>12.0 MB</cell><cell>90.6s</cell><cell>0.42kJ</cell></row><row><cell>GSCv2</cell><cell>Latent+Bit Latent+PQ</cell><cell>0.713 0.708</cell><cell>10.4 MB 11.0 MB</cell><cell>90.8s 95.0s</cell><cell>0.42kJ 0.44kJ</cell></row><row><cell></cell><cell>Latent+Bit+PQ</cell><cell>0.707</cell><cell>10.3 MB</cell><cell>95.2s</cell><cell>0.44kJ</cell></row><row><cell></cell><cell>LifeLearner</cell><cell>0.656</cell><cell>3.40 MB</cell><cell>83.8s</cell><cell>0.39kJ</cell></row><row><cell cols="6">resource overheads in end-to-end latency and energy. Then, Life-</cell></row><row><cell cols="6">Learner, which combines quantization of weights and activations</cell></row><row><cell cols="6">accelerating the CL execution on hardware by exploiting efficient</cell></row><row><cell cols="6">integer-based operations, shows excellent performance in all as-</cell></row><row><cell cols="6">pects: (1) outperforms ANML by a large margin (8.4-22.7%) with a</cell></row><row><cell cols="6">minor accuracy drop compared to Latent (0.9-5.7%), (2) drastically</cell></row><row><cell cols="6">reduces the memory footprint by 61.0-71.2% compared to ANML</cell></row><row><cell cols="6">and by 71.2-73.3% compared to Latent, and (3) incurs minimal over-</cell></row><row><cell cols="6">heads of latency and energy over ANML (costs additional 56.6s and</cell></row><row><cell cols="6">0.3kJ on average, respectively) but still shows lower latency and</cell></row><row><cell cols="6">energy than Latent (saves 47.9s and 0.2kJ on average, respectively).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,317.66,288.99,240.54,123.00"><head>Table 3 :</head><label>3</label><figDesc>MCU deployment of the Backbone, tiny ANML, and tiny LifeLearner on STM32H747.</figDesc><table coords="11,321.60,324.69,233.73,87.30"><row><cell>Dataset</cell><cell>System</cell><cell cols="5">Accuracy SRAM Flash Latency Energy</cell></row><row><cell></cell><cell>Backbone</cell><cell>-</cell><cell>75kB</cell><cell>428kB</cell><cell>561ms</cell><cell>128mJ</cell></row><row><cell>CIFAR-100</cell><cell>Tiny ANML</cell><cell>0.176</cell><cell cols="2">185kB 691kB</cell><cell>579ms</cell><cell>134mJ</cell></row><row><cell></cell><cell>Tiny LifeLearner</cell><cell>0.393</cell><cell cols="2">236kB 825kB</cell><cell>832ms</cell><cell>195mJ</cell></row><row><cell>Mini-ImageNet</cell><cell>Backbone Tiny ANML Tiny LifeLearner</cell><cell>-0.112 0.301</cell><cell cols="3">119kB 329kB 224kB 591kB 281kB 725kB 1204ms 926ms 944ms</cell><cell>221mJ 218mJ 282mJ</cell></row><row><cell></cell><cell>Backbone</cell><cell>-</cell><cell>81kB</cell><cell>475kB</cell><cell>956ms</cell><cell>218mJ</cell></row><row><cell>GSCv2</cell><cell>Tiny ANML</cell><cell>0.209</cell><cell cols="2">181kB 738kB</cell><cell>968ms</cell><cell>223mJ</cell></row><row><cell></cell><cell>Tiny LifeLearner</cell><cell>0.534</cell><cell cols="3">212kB 806kB 1160ms</cell><cell>271mJ</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,320.88,702.79,133.02,6.18"><p>https://github.com/theyoungkwon/LifeLearner</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="12,53.80,62.77,149.48,6.23;12,544.28,62.77,13.92,6.23"><p>SenSys '23, November 12-17, 2023, Istanbul, Turkiye et al.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is supported by a <rs type="funder">Google Faculty Award</rs>, <rs type="funder">ERC</rs> through Project <rs type="grantNumber">833296</rs> (EAR), and <rs type="funder">Nokia Bell Labs</rs> through a donation</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5dXjpqY">
					<idno type="grant-number">833296</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,336.63,240.53,222.33,6.18;12,336.63,248.50,222.75,6.18;12,336.63,256.42,221.57,6.23;12,336.63,264.39,64.30,6.23" xml:id="b0">
	<analytic>
		<title level="a" type="main">Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments</title>
		<author>
			<persName coords=""><forename type="first">Maruan</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yura</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,336.63,272.41,221.57,6.18;12,336.43,280.38,222.85,6.18;12,336.63,288.30,146.34,6.23" xml:id="b1">
	<analytic>
		<title level="a" type="main">Memory Aware Synapses: Learning what (not) to forget</title>
		<author>
			<persName coords=""><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francesca</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,336.63,296.32,222.34,6.18;12,336.63,304.29,222.64,6.18;12,336.63,312.26,222.75,6.18;12,336.63,320.18,221.56,6.23;12,336.63,328.15,66.97,6.23" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Colby</forename><surname>Banbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chuteng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Igor</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramon</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dibakar</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><forename type="middle">Janapa</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Whatmough</surname></persName>
		</author>
		<title level="m">MicroNets: Neural Network Architectures for Deploying TinyML Applications on Commodity Microcontrollers. Proceedings of Machine Learning and Systems (MLSys)</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,336.63,336.17,222.33,6.18;12,336.52,344.09,222.75,6.23;12,336.63,352.11,58.29,6.18" xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to Continually Learn</title>
		<author>
			<persName coords=""><forename type="first">Shawn</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lapo</forename><surname>Frati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Miconi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenneth</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Cheney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI 2020</title>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="992" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,336.63,360.08,221.56,6.18;12,336.63,368.00,221.56,6.23;12,336.63,375.97,137.17,6.23" xml:id="b4">
	<analytic>
		<title level="a" type="main">TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning</title>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,336.63,383.99,221.56,6.18;12,336.63,391.96,222.33,6.18;12,336.63,399.88,221.56,6.23;12,336.63,407.85,120.80,6.23" xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Enable Deep Learning on Mobile Devices: Methods, Systems, and Applications. ACM Transactions on Design Automation of Electronic Systems (TODAES)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">50</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,336.63,415.87,222.33,6.18;12,336.63,423.79,221.57,6.23;12,336.63,431.76,149.04,6.23" xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-End Incremental Learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cordelia</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karteek</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,336.63,439.78,222.75,6.18;12,336.63,447.75,222.75,6.18;12,336.63,455.67,139.62,6.23" xml:id="b7">
	<analytic>
		<title level="a" type="main">Con-tAuth: Continual Learning Framework for Behavioral-based User Authentication</title>
		<author>
			<persName coords=""><forename type="first">Jagmohan</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Young</forename><forename type="middle">D</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cecilia</forename><surname>Mascolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IMWUT</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2020-12">2020. Dec. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,336.63,463.69,221.56,6.18;12,336.63,471.61,221.57,6.23;12,336.63,479.58,222.64,6.23;12,336.63,487.60,100.54,6.18" xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring On-Device Learning Using Few Shots for Audio Classification</title>
		<author>
			<persName coords=""><forename type="first">Jagmohan</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Young</forename><forename type="middle">D</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cecilia</forename><surname>Mascolo</surname></persName>
		</author>
		<idno type="DOI">10.23919/EUSIPCO55093.2022.9909551</idno>
		<ptr target="https://doi.org/10.23919/EUSIPCO55093.2022.9909551" />
	</analytic>
	<monogr>
		<title level="m">2022 30th European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="424" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,336.63,495.57,221.90,6.18;12,336.63,503.54,221.75,6.18;12,336.63,511.46,221.57,6.23;12,336.63,519.43,81.72,6.23" xml:id="b9">
	<analytic>
		<title level="a" type="main">ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training</title>
		<author>
			<persName coords=""><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,336.63,527.45,221.57,6.18;12,336.63,535.42,221.66,6.18" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1604.06174</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1604.06174" />
		<title level="m">Training Deep Nets with Sublinear Memory Cost</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,336.63,543.39,222.34,6.18;12,336.63,551.36,221.56,6.18;12,336.63,559.33,221.70,6.18;12,336.43,567.25,211.86,6.23" xml:id="b11">
	<analytic>
		<title level="a" type="main">TensorFlow Lite Micro: Embedded Machine Learning for TinyML Systems</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jared</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Advait</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><forename type="middle">Janapa</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nat</forename><surname>Jeffries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Kreeger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Nappier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Meghna</forename><surname>Natraj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tiezhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rocky</forename><surname>Rhodes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems (MLSys)</title>
		<meeting>Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,336.63,575.27,222.06,6.18;12,336.63,583.24,221.57,6.18;12,336.63,591.16,221.57,6.23;12,336.63,599.13,222.64,6.23;12,336.47,607.15,74.26,6.18" xml:id="b12">
	<analytic>
		<title level="a" type="main">A Continual Learning Survey: Defying Forgetting in Classification Tasks</title>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><forename type="middle">;</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Slabaugh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3057446</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2021.3057446" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="3366" to="3385" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,336.63,615.12,222.64,6.18;12,336.63,623.04,221.57,6.23;12,336.63,631.01,222.64,6.23;12,336.63,639.03,88.91,6.18" xml:id="b13">
	<analytic>
		<title level="a" type="main">COCOA: Cross Modality Contrastive Learning for Sensor Data</title>
		<author>
			<persName coords=""><forename type="first">Shohreh</forename><surname>Deldari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaqib</forename><surname>Hao Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">V</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Flora</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Salim</surname></persName>
		</author>
		<idno type="DOI">10.1145/3550316</idno>
		<ptr target="https://doi.org/10.1145/3550316" />
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Interact. Mob. Wearable Ubiquitous Technol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2022-09">2022. sep 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,336.63,647.00,221.57,6.18;12,336.63,654.97,222.75,6.18;12,336.63,662.89,221.57,6.23;12,336.63,670.86,58.27,6.23" xml:id="b14">
	<analytic>
		<title level="a" type="main">RF-Net: A Unified Meta-Learning Framework for RF-Enabled One-Shot Human Activity Recognition</title>
		<author>
			<persName coords=""><forename type="first">Shuya</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Embedded Networked Sensor Systems (SenSys &apos;20)</title>
		<meeting>the 18th Conference on Embedded Networked Sensor Systems (SenSys &apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,336.63,678.88,222.33,6.18;12,336.40,686.85,222.57,6.18;12,336.63,694.82,221.56,6.18;12,336.63,702.79,221.57,6.18;13,72.48,89.05,222.02,6.23;13,72.48,97.07,66.10,6.18" xml:id="b15">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName coords=""><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,105.04,222.74,6.18;13,72.48,113.01,222.64,6.18;13,72.48,120.98,185.12,6.18" xml:id="b16">
	<monogr>
		<title level="m" type="main">QN-NPACK: Open source library for optimized mobile deep learning</title>
		<author>
			<persName coords=""><forename type="first">Marat</forename><surname>Dukhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename><surname>Maher</surname></persName>
		</author>
		<ptr target="https://engineering.fb.com/2018/10/29/ml-applications/qnnpack/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,128.95,221.57,6.18;13,72.22,136.87,221.82,6.23;13,72.48,144.84,51.75,6.23" xml:id="b17">
	<analytic>
		<title level="a" type="main">AC-GC: Lossy Activation Compression with Guaranteed Convergence</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tor</forename><surname>Aamodt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,152.86,222.74,6.18;13,72.48,160.78,222.27,6.23;13,72.48,168.75,166.17,6.23" xml:id="b18">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName coords=""><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,176.77,222.33,6.18;13,72.48,184.74,221.56,6.18;13,72.48,192.71,81.23,6.18" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kiseok</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zizheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<title level="m">SqueezeNext: Hardware-Aware Neural Network Design</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1638" to="1647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,200.68,221.56,6.18;13,72.48,208.60,221.56,6.23;13,72.48,216.57,52.46,6.23" xml:id="b20">
	<analytic>
		<title level="a" type="main">Memory-Efficient DNN Training on Mobile Devices</title>
		<author>
			<persName coords=""><forename type="first">In</forename><surname>Gim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeonggil</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Conference on Mobile Systems, Applications and Services (MobiSys)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,224.59,222.74,6.18;13,72.48,232.51,221.57,6.23;13,72.48,240.48,210.80,6.23" xml:id="b21">
	<analytic>
		<title level="a" type="main">MetaSense: Few-Shot Adaptation to Untrained Conditions in Deep Mobile Sensing</title>
		<author>
			<persName coords=""><forename type="first">Taesik</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yeonsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sung-Ju</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference on Embedded Networked Sensor Systems (SenSys &apos;19)</title>
		<meeting>the 17th Conference on Embedded Networked Sensor Systems (SenSys &apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,248.50,220.21,6.18" xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Ga√´l</forename><surname>Guennebaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Beno√Æt</forename><surname>Jacob</surname></persName>
		</author>
		<ptr target="http://eigen.tuxfamily.org" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,256.47,222.33,6.18;13,72.48,264.44,221.56,6.18;13,72.48,272.36,221.57,6.23;13,72.05,280.33,22.27,6.23" xml:id="b23">
	<analytic>
		<title level="a" type="main">A Broader Study of Cross-Domain Few-Shot Learning</title>
		<author>
			<persName coords=""><forename type="first">Yunhui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noel</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">V</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Tajana Rosing, and Rogerio Feris</note>
</biblStruct>

<biblStruct coords="13,72.48,288.35,222.74,6.18;13,72.48,296.32,221.57,6.18;13,72.48,304.24,200.80,6.23" xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</title>
		<author>
			<persName coords=""><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,312.26,221.70,6.18;13,72.48,320.23,222.64,6.18;13,72.48,328.15,146.34,6.23" xml:id="b25">
	<analytic>
		<title level="a" type="main">REMIND Your Neural Network to Prevent Catastrophic Forgetting</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kushal</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robik</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manoj</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,336.17,221.57,6.18;13,72.48,344.09,221.56,6.23;13,72.48,352.06,77.33,6.23" xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,360.08,221.57,6.18;13,72.48,368.00,221.57,6.23;13,72.48,375.97,107.00,6.23" xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparse Bitmap Compression for Memory-Efficient Training on the Edge</title>
		<author>
			<persName coords=""><forename type="first">Abdelrahman</forename><surname>Hosny</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marina</forename><surname>Neseem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sherief</forename><surname>Reda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM Symposium on Edge Computing (SEC)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,383.99,222.64,6.18;13,72.48,391.91,221.57,6.23;13,72.48,399.88,222.02,6.23;13,72.32,407.90,221.73,6.18;13,72.23,415.87,99.56,6.18" xml:id="b28">
	<analytic>
		<title level="a" type="main">Meta-Learning in Neural Networks: A Survey</title>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3079209</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2021.3079209" />
	</analytic>
	<monogr>
		<title level="m">Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022-09">2022. Sept. 2022</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="5149" to="5169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,423.84,222.64,6.18;13,72.48,431.81,221.56,6.18;13,72.48,439.73,221.57,6.23;13,72.48,447.70,89.49,6.23" xml:id="b29">
	<analytic>
		<title level="a" type="main">Pushing the Limits of Simple Pipelines for Few-Shot Learning: External Data and Fine-Tuning Make a Difference</title>
		<author>
			<persName coords=""><forename type="first">Shell</forename><surname>Xu Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>St√ºhmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,455.72,221.57,6.18;13,72.48,463.64,221.57,6.23;13,72.48,471.61,221.57,6.23;13,72.48,479.58,52.16,6.23" xml:id="b30">
	<analytic>
		<title level="a" type="main">SwapAdvisor: Pushing Deep Learning Beyond the GPU Memory Limit via Smart Swapping</title>
		<author>
			<persName coords=""><forename type="first">Chien-Chin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,487.60,221.56,6.18;13,72.48,495.52,221.57,6.23;13,72.48,503.49,221.56,6.23;13,72.48,511.46,62.43,6.23" xml:id="b31">
	<analytic>
		<title level="a" type="main">ElasticTrainer: Speeding Up On-Device Training with Runtime Elastic Tensor Selection</title>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services (MobiSys &apos;23)</title>
		<meeting>the 21st Annual International Conference on Mobile Systems, Applications and Services (MobiSys &apos;23)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,519.48,222.33,6.18;13,72.48,527.45,221.56,6.18;13,72.48,535.42,221.57,6.18;13,72.48,543.34,221.57,6.23;13,72.48,551.31,51.75,6.23" xml:id="b32">
	<analytic>
		<title level="a" type="main">GPipe: Efficient Training of Giant Neural Networks Using Pipeline Parallelism</title>
		<author>
			<persName coords=""><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,559.33,221.56,6.18;13,72.48,567.30,222.75,6.18;13,72.48,575.22,221.56,6.23;13,72.05,583.19,47.03,6.23" xml:id="b33">
	<analytic>
		<title level="a" type="main">Compacting, Picking and Growing for Unforgetting Continual Learning</title>
		<author>
			<persName coords=""><forename type="first">Ching-Yi</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cheng-Hao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cheng-En</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chien-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi-Ming</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chu-Song</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,591.21,222.74,6.18;13,72.48,599.18,221.57,6.18;13,72.48,607.15,222.74,6.18;13,72.48,615.07,221.56,6.23;13,72.48,623.04,55.54,6.23" xml:id="b34">
	<analytic>
		<title level="a" type="main">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</title>
		<author>
			<persName coords=""><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,631.06,221.57,6.18;13,72.48,639.03,221.75,6.18;13,72.14,646.95,221.90,6.23;13,72.48,654.92,60.11,6.23" xml:id="b35">
	<analytic>
		<title level="a" type="main">Checkmate: Breaking the Memory Wall with Optimal Tensor Rematerialization</title>
		<author>
			<persName coords=""><forename type="first">Paras</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aniruddha</forename><surname>Nrusimha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Machine Learning and Systems (MLSys)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,662.94,221.70,6.18;13,72.48,670.86,221.57,6.23;13,72.05,678.83,28.01,6.23" xml:id="b36">
	<analytic>
		<title level="a" type="main">Meta-Learning Representations for Continual Learning</title>
		<author>
			<persName coords=""><forename type="first">Khurram</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martha</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.48,686.85,222.34,6.18;13,72.28,694.82,221.77,6.18;13,72.48,702.74,221.56,6.23;13,336.63,89.05,221.57,6.23;13,335.89,97.02,11.78,6.23" xml:id="b37">
	<analytic>
		<title level="a" type="main">Band: Coordinated Multi-DNN Inference on Heterogeneous Mobile Processors</title>
		<author>
			<persName coords=""><forename type="first">Joo</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeong</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Changmin</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Changjin</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Youngki</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Byung-Gon</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services (MobiSys &apos;22)</title>
		<meeting>the 20th Annual International Conference on Mobile Systems, Applications and Services (MobiSys &apos;22)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,105.04,221.57,6.18;13,336.63,113.01,221.75,6.18;13,336.63,120.93,222.64,6.23;13,336.63,128.95,42.18,6.18" xml:id="b38">
	<analytic>
		<title level="a" type="main">Continual learning in sensor-based human activity recognition: An empirical benchmark analysis</title>
		<author>
			<persName coords=""><forename type="first">Saurav</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Schiemer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Franco</forename><surname>Zambonelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juan</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2021.04.062</idno>
		<ptr target="https://doi.org/10.1016/j.ins.2021.04.062" />
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">575</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2021-10">2021. Oct. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,136.92,221.75,6.18;13,336.63,144.84,173.47,6.23" xml:id="b39">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with GPUs</title>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Herv√©</forename><surname>J√©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,152.86,221.56,6.18;13,336.63,160.78,221.57,6.23;13,336.63,168.80,72.46,6.18" xml:id="b40">
	<analytic>
		<title level="a" type="main">Product Quantization for Nearest Neighbor Search</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>J√©gou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2011-01">2011. Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,176.77,221.56,6.18;13,336.63,184.69,217.58,6.23" xml:id="b41">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,192.71,222.33,6.18;13,336.52,200.68,222.86,6.18;13,336.63,208.60,214.30,6.23" xml:id="b42">
	<analytic>
		<title level="a" type="main">Dynamic Tensor Rematerialization</title>
		<author>
			<persName coords=""><forename type="first">Marisa</forename><surname>Kirisame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Lyubomirsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Altan</forename><surname>Haan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jennifer</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jared</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zachary</forename><surname>Tatlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,216.62,221.56,6.18;13,336.63,224.59,221.57,6.18;13,336.63,232.56,221.57,6.18;13,336.63,240.53,222.64,6.18;13,336.63,248.45,192.61,6.23" xml:id="b43">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Claudia Clopath, Dharshan Kumaran, and Raia Hadsell</title>
		<imprint>
			<date type="published" when="2017-03">2017. March 2017</date>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,256.47,222.75,6.18;13,336.63,264.44,221.57,6.18;13,336.63,272.36,221.70,6.23;13,336.63,280.38,138.36,6.18" xml:id="b44">
	<analytic>
		<title level="a" type="main">Lane Compression: A Lightweight Lossless Compression Method for Machine Learning on Embedded Systems</title>
		<author>
			<persName coords=""><forename type="first">Yousun</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Chadwick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Mullins</surname></persName>
		</author>
		<idno type="DOI">10.1145/3431815</idno>
		<ptr target="https://doi.org/10.1145/3431815" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Embed. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2021-03">2021. mar 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,288.35,221.57,6.18;13,336.63,296.27,213.58,6.23" xml:id="b45">
	<monogr>
		<title level="m" type="main">Quantizing deep convolutional networks for efficient inference: A whitepaper</title>
		<author>
			<persName coords=""><forename type="first">Raghuraman</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08342</idno>
		<imprint>
			<date type="published" when="2018-06">2018. June 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct coords="13,336.63,304.29,221.57,6.18;13,336.63,312.26,95.78,6.18" xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,320.23,221.56,6.18;13,336.63,328.20,222.75,6.18;13,336.63,336.12,221.57,6.23;13,336.63,344.09,166.21,6.23" xml:id="b47">
	<analytic>
		<title level="a" type="main">Exploring System Performance of Continual Learning for Mobile and Embedded Sensing Applications</title>
		<author>
			<persName coords=""><forename type="first">Jagmohan</forename><surname>Young D Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abhishek</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cecilia</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mascolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE Symposium on Edge Computing</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,352.11,222.39,6.18;13,336.63,360.08,221.57,6.18;13,336.39,368.00,216.88,6.23" xml:id="b48">
	<analytic>
		<title level="a" type="main">FastICARL: Fast Incremental Classifier and Representation Learning with Efficient Budget Allocation in Audio Sensing Applications</title>
		<author>
			<persName coords=""><forename type="first">Young</forename><forename type="middle">D</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jagmohan</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cecilia</forename><surname>Mascolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2021</title>
		<meeting>Interspeech 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="356" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,376.02,222.75,6.18;13,336.63,383.94,221.57,6.23;13,336.63,391.91,221.57,6.23;13,336.21,399.88,178.68,6.23" xml:id="b49">
	<analytic>
		<title level="a" type="main">YONO: Modeling Multiple Heterogeneous Neural Networks on Microcontrollers</title>
		<author>
			<persName coords=""><forename type="first">Young</forename><forename type="middle">D</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jagmohan</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cecilia</forename><surname>Mascolo</surname></persName>
		</author>
		<idno type="DOI">10.1109/IPSN54338.2022.00030</idno>
		<ptr target="https://doi.org/10.1109/IPSN54338.2022.00030" />
	</analytic>
	<monogr>
		<title level="m">2022 21st ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="285" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,407.90,222.64,6.18;13,336.63,415.87,221.57,6.18;13,336.63,423.84,125.50,6.18" xml:id="b50">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Young</forename><forename type="middle">D</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stylianos</forename><forename type="middle">I</forename><surname>Venieris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jagmohan</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cecilia</forename><surname>Mascolo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09988[cs.LG]</idno>
		<title level="m">TinyTrain: Deep Neural Network Training at the Extreme Edge</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,431.81,222.64,6.18;13,336.63,439.73,222.27,6.23;13,336.63,447.70,221.57,6.23;13,336.63,455.72,179.78,6.18" xml:id="b51">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aab3050</idno>
		<ptr target="https://www.science.org/doi/pdf/10.1126/science.aab3050" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,463.69,222.39,6.18;13,336.63,471.66,222.64,6.18;13,336.63,479.58,221.57,6.23;13,336.21,487.55,34.37,6.23" xml:id="b52">
	<analytic>
		<title level="a" type="main">GazeGraph: Graph-Based Few-Shot Cognitive Context Sensing from Human Visual Behavior</title>
		<author>
			<persName coords=""><forename type="first">Guohao</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bailey</forename><surname>Heit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Scargill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Gorlatova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Embedded Networked Sensor Systems (SenSys &apos;20)</title>
		<meeting>the 18th Conference on Embedded Networked Sensor Systems (SenSys &apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,495.57,222.75,6.18;13,336.63,503.49,221.57,6.23;13,336.63,511.46,153.17,6.23" xml:id="b53">
	<analytic>
		<title level="a" type="main">A survey of mobile phone sensing</title>
		<author>
			<persName coords=""><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emiliano</forename><surname>Miluzzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tanzeem</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">T</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="140" to="150" />
			<date type="published" when="2010-09">2010. Sept. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,519.48,221.57,6.18;13,336.63,527.40,221.75,6.23;13,336.63,535.42,108.68,6.18" xml:id="b54">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cheng-Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen-Yi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.14053</idno>
		<ptr target="http://arxiv.org/abs/2107.14053" />
		<title level="m">Few-Shot and Continual Learning with Attentive Independent Mechanisms</title>
		<imprint>
			<date type="published" when="2021-07">2021. July 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,543.39,221.57,6.18;13,336.63,551.31,222.26,6.23;13,336.63,559.28,157.67,6.23" xml:id="b55">
	<analytic>
		<title level="a" type="main">uNAS: Constrained Neural Architecture Search for Microcontrollers</title>
		<author>
			<persName coords=""><forename type="first">Edgar</forename><surname>Liberis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">≈Åukasz</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Machine Learning and Systems (EuroMLSys &apos;21)</title>
		<meeting>the 1st Workshop on Machine Learning and Systems (EuroMLSys &apos;21)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,567.30,222.64,6.18;13,336.63,575.22,221.57,6.23;13,336.63,583.19,81.87,6.23" xml:id="b56">
	<analytic>
		<title level="a" type="main">MCUNet: Tiny Deep Learning on IoT Devices</title>
		<author>
			<persName coords=""><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,591.21,221.57,6.18;13,336.63,599.13,221.57,6.23;13,336.63,607.10,116.62,6.23" xml:id="b57">
	<analytic>
		<title level="a" type="main">On-Device Training Under 256KB Memory</title>
		<author>
			<persName coords=""><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,615.12,221.57,6.18;13,336.40,623.09,221.79,6.18;13,336.63,631.01,221.56,6.23;13,336.63,638.98,67.69,6.23" xml:id="b58">
	<analytic>
		<title level="a" type="main">BlastNet: Exploiting Duo-Blocks for Cross-Processor Real-Time DNN Inference</title>
		<author>
			<persName coords=""><forename type="first">Neiwen</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhihe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhenyu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guoliang</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems (SenSys)</title>
		<meeting>the 20th ACM Conference on Embedded Networked Sensor Systems (SenSys)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,647.00,222.74,6.18;13,336.63,654.97,221.57,6.18;13,336.63,662.89,221.57,6.23;13,336.21,670.86,34.37,6.23" xml:id="b59">
	<analytic>
		<title level="a" type="main">RT-MDL: Supporting Real-Time Mixed Deep Learning Tasks on Edge Platforms</title>
		<author>
			<persName coords=""><forename type="first">Neiwen</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuze</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guoliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daqi</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems (SenSys &apos;21)</title>
		<meeting>the 19th ACM Conference on Embedded Networked Sensor Systems (SenSys &apos;21)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,336.63,678.88,221.56,6.18;13,336.43,686.85,221.95,6.18;13,336.63,694.77,221.57,6.23;13,336.63,702.74,81.13,6.23" xml:id="b60">
	<analytic>
		<title level="a" type="main">AutoCompress: An Automatic DNN Structured Pruning Framework for Ultra-High Compression Rates</title>
		<author>
			<persName coords=""><forename type="first">Ning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaolong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.48,89.10,222.33,6.18;14,72.37,97.07,221.68,6.18;14,72.48,105.04,221.75,6.18;14,72.23,112.96,201.12,6.23" xml:id="b61">
	<analytic>
		<title level="a" type="main">GACT: Activation Compressed Training for Generic Network Architectures</title>
		<author>
			<persName coords=""><forename type="first">Xiaoxuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weize</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joey</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.48,120.98,221.56,6.18;14,72.48,128.90,152.87,6.23" xml:id="b62">
	<monogr>
		<title level="m" type="main">Gradient Episodic Memory for Continual Learning</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc\textquotesingle</forename><surname>Aurelio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ranzato</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct coords="14,72.48,136.92,221.56,6.18;14,72.25,144.84,221.80,6.23;14,72.48,152.81,110.89,6.23" xml:id="b63">
	<analytic>
		<title level="a" type="main">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</title>
		<author>
			<persName coords=""><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.48,160.83,221.75,6.18;14,72.48,168.80,222.39,6.18;14,72.48,176.77,221.57,6.18;14,72.48,184.69,151.00,6.23" xml:id="b64">
	<analytic>
		<title level="a" type="main">Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory</title>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><forename type="middle">L</forename><surname>Mcnaughton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Randall</forename><forename type="middle">C</forename><surname>O'reilly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="419" to="457" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.48,192.71,221.56,6.18;14,72.48,200.63,221.57,6.23;14,72.48,208.60,119.41,6.23" xml:id="b65">
	<analytic>
		<title level="a" type="main">Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Learning and Motivation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.48,216.62,221.57,6.18;14,72.48,224.54,221.57,6.23;14,72.48,232.51,146.44,6.23" xml:id="b66">
	<analytic>
		<title level="a" type="main">Essentials for Class Incremental Learning</title>
		<author>
			<persName coords=""><forename type="first">Sudhanshu</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Silvio</forename><surname>Galesso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.48,240.53,222.75,6.18;14,72.48,248.45,222.27,6.23;14,72.48,256.42,149.18,6.23" xml:id="b67">
	<analytic>
		<title level="a" type="main">Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL</title>
		<author>
			<persName coords=""><forename type="first">Anusha</forename><surname>Nagabandi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.48,264.39,221.57,6.23;14,72.48,272.36,219.83,6.23" xml:id="b68">
	<analytic>
		<title level="a" type="main">A Survey on Transfer Learning</title>
		<author>
			<persName coords=""><forename type="first">Jialin</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010-10">2010. Oct. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.48,280.38,222.64,6.18;14,72.48,288.30,221.57,6.23;14,72.48,296.27,90.96,6.23" xml:id="b69">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Zizheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haoyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11124</idno>
		<title level="m">Mesa: A Memory-saving Training Framework for Transformers</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,72.48,304.29,221.57,6.18;14,72.14,312.26,222.98,6.18;14,72.48,320.18,112.82,6.23" xml:id="b70">
	<analytic>
		<title level="a" type="main">Continual lifelong learning with neural networks: A review</title>
		<author>
			<persName coords=""><forename type="first">German</forename><forename type="middle">I</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ronald</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jose</forename><forename type="middle">L</forename><surname>Part</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="54" to="71" />
			<date type="published" when="2019-05">2019. May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.48,328.20,222.74,6.18;14,72.48,336.17,222.33,6.18;14,72.23,344.14,222.99,6.18;14,72.48,352.11,222.34,6.18;14,72.48,360.08,221.57,6.18;14,72.48,368.00,221.57,6.23;14,72.05,375.97,28.01,6.23" xml:id="b71">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.48,383.99,222.64,6.18;14,72.48,391.96,222.74,6.18;14,72.48,399.88,215.48,6.23" xml:id="b72">
	<analytic>
		<title level="a" type="main">POET: Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging</title>
		<author>
			<persName coords=""><forename type="first">Paras</forename><surname>Shishir G Patil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prabal</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ion</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.48,407.90,222.64,6.18;14,72.48,415.82,221.57,6.23;14,72.48,423.79,145.59,6.23" xml:id="b73">
	<analytic>
		<title level="a" type="main">Latent Replay for Real-Time Continual Learning</title>
		<author>
			<persName coords=""><forename type="first">Lorenzo</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriele</forename><surname>Graffieti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincenzo</forename><surname>Lomonaco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Davide</forename><surname>Maltoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.48,431.81,221.56,6.18;14,72.48,439.78,221.56,6.18;14,72.48,447.75,222.74,6.18;14,72.48,455.67,221.57,6.23;14,72.48,463.64,222.64,6.23;14,72.48,471.66,24.81,6.18" xml:id="b74">
	<analytic>
		<title level="a" type="main">PROS: An Efficient Pattern-Driven Compressive Sensing Framework for Low-Power Biopotential-Based Wearables with on-Chip Intelligence</title>
		<author>
			<persName coords=""><forename type="first">Nhat</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tuan</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nam</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Young</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Phuc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cecilia</forename><surname>Mascolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tam</forename><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International Conference on Mobile Computing And Networking</title>
		<meeting>the 28th Annual International Conference on Mobile Computing And Networking</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="661" to="675" />
		</imprint>
	</monogr>
	<note>MobiCom &apos;22</note>
</biblStruct>

<biblStruct coords="14,72.48,479.63,222.64,6.18;14,72.27,487.60,222.96,6.18;14,72.48,495.52,221.57,6.23;14,72.48,503.49,211.15,6.23" xml:id="b75">
	<analytic>
		<title level="a" type="main">Computationally Budgeted Continual Learning: What Does Matter?</title>
		<author>
			<persName coords=""><forename type="first">Ameya</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abed</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kader</forename><surname>Al</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Puneet</forename><forename type="middle">K</forename><surname>Hammoud</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ser-Nam</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernard</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adel</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3698" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.48,511.51,221.57,6.18;14,72.27,519.43,221.78,6.23;14,72.48,527.40,173.27,6.23" xml:id="b76">
	<analytic>
		<title level="a" type="main">Deep Learning for Audio Signal Processing</title>
		<author>
			<persName coords=""><forename type="first">Hendrik</forename><surname>Purwins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tuomas</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Schl√ºter</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Shuo-Yiin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tara</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="206" to="219" />
			<date type="published" when="2019-05">2019. May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.48,535.42,222.39,6.18;14,72.27,543.34,221.78,6.23;14,72.48,551.31,222.33,6.23;14,72.48,559.28,222.33,6.23;14,72.32,567.30,149.67,6.18" xml:id="b77">
	<analytic>
		<title level="a" type="main">P-Meta: Towards On-Device Deep Model Adaptation</title>
		<author>
			<persName coords=""><forename type="first">Zhongnan</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zimu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongxin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lothar</forename><surname>Thiele</surname></persName>
		</author>
		<idno type="DOI">10.1145/3534678.3539293</idno>
		<ptr target="https://doi.org/10.1145/3534678.3539293" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining<address><addrLine>Washington DC, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.48,575.27,221.57,6.18;14,72.48,583.24,222.64,6.18;14,72.48,591.16,176.77,6.23" xml:id="b78">
	<analytic>
		<title level="a" type="main">Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML</title>
		<author>
			<persName coords=""><forename type="first">Aniruddh</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Samy Bengio, and Oriol Vinyals</note>
</biblStruct>

<biblStruct coords="14,72.48,599.18,222.64,6.18;14,72.25,607.15,222.97,6.18;14,72.22,615.07,222.59,6.23;14,72.48,623.09,218.01,6.18" xml:id="b79">
	<analytic>
		<title level="a" type="main">XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.47,631.06,221.57,6.18;14,72.48,639.03,221.56,6.18;14,72.48,646.95,65.45,6.23" xml:id="b80">
	<analytic>
		<title level="a" type="main">iCaRL: Incremental classifier and representation learning</title>
		<author>
			<persName coords=""><surname>Sylvestre-Alvise</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2001">2017. 2001-2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.48,654.97,221.57,6.18;14,72.48,662.94,222.75,6.18;14,72.48,670.86,183.39,6.23" xml:id="b81">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neil</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<title level="m">Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,72.48,678.88,222.74,6.18;14,72.48,686.85,221.56,6.18;14,72.48,694.77,219.79,6.23" xml:id="b82">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,336.63,89.10,222.75,6.18;14,336.63,97.07,221.57,6.18;14,336.63,104.99,221.57,6.23;14,336.63,112.96,113.83,6.23" xml:id="b83">
	<analytic>
		<title level="a" type="main">Progress &amp; Compress: A Scalable Framework for Continual Learning</title>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jelena</forename><surname>Luketina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,336.63,120.98,221.57,6.18;14,336.38,128.95,221.82,6.18;14,336.63,136.87,111.53,6.23" xml:id="b84">
	<monogr>
		<title level="m" type="main">Knowing when we do not know: Bayesian continual learning for sensing-based analysis tasks</title>
		<author>
			<persName coords=""><forename type="first">Sandra</forename><surname>Servia-Rodriguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cecilia</forename><surname>Mascolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Young</forename><forename type="middle">D</forename><surname>Kwon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05872</idno>
		<imprint>
			<date type="published" when="2021-06">2021. June 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,336.63,144.89,221.56,6.18;14,336.63,152.81,221.57,6.23;14,336.21,160.78,28.01,6.23" xml:id="b85">
	<analytic>
		<title level="a" type="main">Prototypical Networks for Few-shot Learning</title>
		<author>
			<persName coords=""><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,336.63,168.80,221.57,6.18;14,336.63,176.77,221.81,6.18;14,336.43,184.69,160.72,6.23" xml:id="b86">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Sharad</forename><surname>Nimit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Sohoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Megan</forename><surname>Richard Aberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Leszczynski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>R√©</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10631</idno>
		<title level="m">Low-Memory Neural Network Training: A</title>
		<imprint>
			<date type="published" when="2019-04">2019. April 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>cs, stat</note>
</biblStruct>

<biblStruct coords="14,336.63,192.71,222.33,6.18;14,336.63,200.68,221.70,6.18;14,336.63,208.60,222.26,6.23;14,336.63,216.57,40.12,6.23" xml:id="b87">
	<analytic>
		<title level="a" type="main">Training with Quantization Noise for Extreme Model Compression</title>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R√©mi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct coords="14,336.63,224.59,221.56,6.18;14,336.52,232.56,221.68,6.18;14,336.63,240.48,207.79,6.23" xml:id="b88">
	<analytic>
		<title level="a" type="main">And the Bit Goes Down: Revisiting the Quantization of Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R√©mi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Herv√©</forename><surname>J√©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,336.63,248.50,222.64,6.18;14,336.63,256.47,221.57,6.18;14,336.63,264.39,221.57,6.23;14,336.63,272.36,82.93,6.23" xml:id="b89">
	<analytic>
		<title level="a" type="main">Deep Learning on Microcontrollers: A Study on Deployment Costs and Challenges</title>
		<author>
			<persName coords=""><forename type="first">Filip</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Javier</forename><surname>Fernandez-Marques</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edgar</forename><surname>Liberis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd European Workshop on Machine Learning and Systems (EuroMLSys &apos;22)</title>
		<meeting>the 2nd European Workshop on Machine Learning and Systems (EuroMLSys &apos;22)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,336.63,280.38,221.57,6.18;14,336.63,288.30,222.64,6.23;14,336.63,296.32,221.57,6.18;14,336.63,304.29,26.07,6.18" xml:id="b90">
	<analytic>
		<title level="a" type="main">Efficient Processing of Deep Neural Networks: A Tutorial and Survey</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2017.2761740</idno>
		<ptr target="https://doi.org/10.1109/JPROC.2017.2761740" />
	</analytic>
	<monogr>
		<title level="m">Conference Name: Proceedings of the IEEE</title>
		<imprint>
			<date type="published" when="2017-12">2017. Dec. 2017</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2295" to="2329" />
		</imprint>
	</monogr>
	<note>Proc. IEEE 105</note>
</biblStruct>

<biblStruct coords="14,336.63,312.26,221.56,6.18;14,336.52,320.23,221.68,6.18;14,336.63,328.20,139.73,6.18" xml:id="b91">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Subin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sihyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaeho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><forename type="middle">Richard</forename><surname>Schwarz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.00617[cs.LG]</idno>
		<title level="m">Learning Large-scale Neural Fields via Context Pruned Meta-Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,336.63,336.17,222.34,6.18;14,336.63,344.14,222.75,6.18;14,336.63,352.11,221.70,6.18;14,336.63,360.03,221.57,6.23;14,336.63,368.00,64.30,6.23" xml:id="b92">
	<analytic>
		<title level="a" type="main">Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples</title>
		<author>
			<persName coords=""><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,336.63,376.02,222.33,6.18;14,336.39,383.99,221.81,6.18;14,336.63,391.91,221.57,6.23;14,336.63,399.93,221.56,6.18;14,336.43,407.90,222.12,6.18;14,336.63,415.87,189.92,6.18" xml:id="b93">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">≈Å Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2017/file/3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</note>
</biblStruct>

<biblStruct coords="14,336.63,423.84,221.56,6.18;14,336.63,431.76,221.57,6.23;14,336.63,439.73,144.46,6.23" xml:id="b94">
	<analytic>
		<title level="a" type="main">Matching Networks for One Shot Learning</title>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,336.63,447.75,221.57,6.18;14,336.63,455.72,221.57,6.18;14,336.63,463.64,221.57,6.23;14,336.63,471.61,186.84,6.23" xml:id="b95">
	<analytic>
		<title level="a" type="main">Melon: Breaking the Memory Wall for Resource-Efficient On-Device Machine Learning</title>
		<author>
			<persName coords=""><forename type="first">Qipeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mengwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinran</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinliang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yunxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanzhe</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Conference on Mobile Systems, Applications and Services (MobiSys)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,336.63,479.63,221.75,6.18;14,336.63,487.55,155.24,6.23" xml:id="b96">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<title level="m">Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition</title>
		<imprint>
			<date type="published" when="2018-04">2018. April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,336.63,495.57,221.56,6.18;14,336.63,503.49,221.57,6.23;14,336.63,511.46,206.48,6.23" xml:id="b97">
	<analytic>
		<title level="a" type="main">Large Scale Incremental Learning</title>
		<author>
			<persName coords=""><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuancheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,336.63,519.48,222.75,6.18;14,336.63,527.40,221.57,6.23;14,336.63,535.37,206.31,6.23" xml:id="b98">
	<analytic>
		<title level="a" type="main">OneFi: One-Shot Recognition for Unseen Gesture via COTS WiFi</title>
		<author>
			<persName coords=""><forename type="first">Rui</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinsong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kui</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems (SenSys &apos;21)</title>
		<meeting>the 19th ACM Conference on Embedded Networked Sensor Systems (SenSys &apos;21)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="206" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,336.63,543.39,221.57,6.18;14,336.63,551.36,221.56,6.18;14,336.63,559.33,222.64,6.18;14,336.63,567.25,221.57,6.23;14,336.21,575.22,60.92,6.23" xml:id="b99">
	<analytic>
		<title level="a" type="main">Deep Compressive Offloading: Speeding up Neural Network Inference by Trading Edge Computation for Network Latency</title>
		<author>
			<persName coords=""><forename type="first">Shuochao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianshi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huajie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tarek</forename><surname>Abdelzaher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Embedded Networked Sensor Systems (SenSys &apos;20</title>
		<meeting>the 18th Conference on Embedded Networked Sensor Systems (SenSys &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="476" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,336.63,583.24,221.57,6.18;14,336.63,591.16,221.57,6.23;14,336.63,599.13,99.29,6.23" xml:id="b100">
	<analytic>
		<title level="a" type="main">Lifelong Learning with Dynamically Expandable Networks</title>
		<author>
			<persName coords=""><forename type="first">Jaehong</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeongtae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,336.63,607.15,222.64,6.18;14,336.63,615.12,222.64,6.18;14,336.63,623.09,70.39,6.18" xml:id="b101">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Sheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ju</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiang</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaoxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weihua</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.06453[cs.LG]</idno>
		<title level="m">Efficient Federated Meta-Learning over Multi-Access Wireless Networks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,336.63,631.06,221.56,6.18;14,336.43,638.98,162.25,6.23" xml:id="b102">
	<analytic>
		<title level="a" type="main">Continual Learning Through Synaptic Intelligence</title>
		<author>
			<persName coords=""><forename type="first">Friedemann</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3987" to="3995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,336.63,647.00,221.57,6.18;14,336.63,654.97,222.75,6.18;14,336.46,662.89,221.74,6.23;14,336.63,670.86,144.33,6.23" xml:id="b103">
	<analytic>
		<title level="a" type="main">MDLdroidLite: A Release-and-Inhibit Control Approach to Resource-Efficient Deep Neural Networks on Mobile Devices</title>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Embedded Networked Sensor Systems (SenSys &apos;20)</title>
		<meeting>the 18th Conference on Embedded Networked Sensor Systems (SenSys &apos;20)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="463" to="475" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
